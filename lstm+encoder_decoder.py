# -*- coding: utf-8 -*-
"""LSTM+ENCODER-DECODER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kpaf8WRT86bGhaGMqj7-z_nueqTWCZst
"""

pip install tensorflow

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer



df = pd.read_csv('FidelFolio_Dataset.csv')

df.shape

df.info()

df.isnull().sum()

df.columns



"""## EDA"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- Step 1: Clean column names ---
df.columns = df.columns.str.strip()

# --- Step 2: Convert numeric-looking strings to numeric ---
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
        df[col] = pd.to_numeric(df[col], errors='ignore')

for col in df.select_dtypes(include='object').columns:
    converted = pd.to_numeric(df[col], errors='coerce')
    if converted.notnull().sum() > 0:
        df[col] = converted

# --- Step 3: Summary statistics ---
print("Summary Statistics:\n", df.describe())

# --- Step 4: Missing values ---
print("\nMissing Values:\n", df.isnull().sum())

# --- Step 5: Correlation Heatmap (features + targets) ---
numeric_df = df.select_dtypes(include='number')
plt.figure(figsize=(16, 12))
sns.heatmap(numeric_df.corr(), annot=False, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.show()

# --- Step 6: Distribution of Targets ---
target_cols = ['Target 1', 'Target 2', 'Target 3']
for col in target_cols:
    if col in df.columns:
        plt.figure(figsize=(6, 4))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f"Distribution of {col}")
        plt.tight_layout()
        plt.show()


# --- Step 9: Feature vs Target scatter plots ---
feature_cols = [col for col in df.columns if col.startswith("Feature")]
for target in target_cols:
    for feature in feature_cols[:5]:  # Only first 5 to avoid too many plots
        plt.figure(figsize=(6, 4))
        sns.scatterplot(data=df, x=feature, y=target, hue='Company')
        plt.title(f"{feature} vs {target}")
        plt.tight_layout()
        plt.show()

import pandas as pd
import plotly.graph_objects as go
import plotly.express as px

# Step 1: Clean column names
df.columns = df.columns.str.strip()

# Step 2: Convert numeric-looking strings to numeric
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
        df[col] = pd.to_numeric(df[col], errors='ignore')

for col in df.select_dtypes(include='object').columns:
    converted = pd.to_numeric(df[col], errors='coerce')
    if converted.notnull().sum() > 0:
        df[col] = converted

# Step 3: Prepare the dropdown-based interactive plot
company_list = df['Company'].unique()
target_cols = ['Target 1', 'Target 2', 'Target 3']

# Step 4: Create figure with traces for each company
fig = go.Figure()

# Add one trace per target for each company
for company in company_list:
    filtered_df = df[df['Company'] == company].sort_values('Year')
    for target in target_cols:
        fig.add_trace(go.Scatter(
            x=filtered_df['Year'],
            y=filtered_df[target],
            mode='lines+markers',
            name=f"{target} - {company}",
            visible=(company == company_list[0])  # Only show first company by default
        ))

# Step 5: Create dropdown menu
dropdown_buttons = []
for i, company in enumerate(company_list):
    visible = [False] * len(company_list) * len(target_cols)
    for j in range(len(target_cols)):
        visible[i * len(target_cols) + j] = True
    dropdown_buttons.append(dict(
        label=company,
        method='update',
        args=[{'visible': visible},
              {'title': f"Year-on-Year Growth for {company}"}]
    ))

# Step 6: Final layout
fig.update_layout(
    updatemenus=[dict(
        active=0,
        buttons=dropdown_buttons,
        x=0.1,
        y=1.15,
        xanchor='left',
        yanchor='top'
    )],
    title=f"Year-on-Year Growth for {company_list[0]}",
    xaxis_title="Year",
    yaxis_title="Target Value",
    height=600
)

fig.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# --- Step 1: Clean column names ---
df.columns = df.columns.str.strip()

# --- Step 2: Remove commas and convert strings to numeric where applicable ---
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
        df[col] = pd.to_numeric(df[col], errors='ignore')

# --- Step 3: Attempt numeric conversion again with coercion for remaining object columns ---
for col in df.select_dtypes(include='object').columns:
    converted = pd.to_numeric(df[col], errors='coerce')
    if converted.notnull().sum() > 0:
        df[col] = converted

# --- Step 4: Separate numeric and non-numeric columns ---
numeric_df = df.select_dtypes(include='number').copy()
non_numeric_df = df.drop(columns=numeric_df.columns)

# --- Step 5: Drop numeric columns that are entirely NaN ---
numeric_df = numeric_df.dropna(axis=1, how='all')

# --- Step 6: Full Correlation Heatmap ---
plt.figure(figsize=(16, 12))
sns.heatmap(numeric_df.corr(), annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title("Correlation Heatmap (All Numeric Columns)")
plt.tight_layout()
plt.show()

# --- Step 7: Feature vs Target Correlation Heatmap ---
feature_cols = [col for col in numeric_df.columns if col.startswith("Feature")]
target_cols = [col for col in numeric_df.columns if "Target" in col]

# Only proceed if target columns exist
if feature_cols and target_cols:
    feature_target_corr = numeric_df[feature_cols + target_cols].corr().loc[feature_cols, target_cols]

    plt.figure(figsize=(8, 14))
    sns.heatmap(feature_target_corr, annot=True, fmt=".2f", cmap='coolwarm')
    plt.title("Feature vs Target Correlation")
    plt.xlabel("Targets")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()
else:
    print("Feature or Target columns not found in numeric_df.")

pip install tensorflow

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler, LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding,
                                     Concatenate, Masking, Flatten)
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping
import warnings

# --- Configuration ---
# Preprocessing
N_NEIGHBORS_IMPUTE = 5      # For KNNImputer
Z_SCORE_CAP = 3          # For outlier capping after scaling (applied to scaled data)
# Model Hyperparameters
EMBEDDING_DIM = 10       # Dimension for company embedding
LSTM_UNITS = 64          # Number of units in the LSTM layer
DENSE_UNITS = 32         # Number of units in the dense layer after LSTM/Embedding merging
# Training Parameters
EPOCHS = 50              # Max number of training epochs per fold
BATCH_SIZE = 64          # Batch size for training
VALIDATION_SPLIT = 0.15  # Fraction of training data (in each fold) used for validation
EARLY_STOPPING_PATIENCE = 7 # Patience for early stopping (epochs without val_rmse improvement)
MIN_HISTORY_YEARS = 2    # Minimum years of history required for a company to make a prediction

# Ignore SettingWithCopyWarning, common in pandas operations within loops/functions
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)

# --- Load Data ---

# Verify essential columns exist
required_cols = ['Company', 'Year', 'Target 1', 'Target 2', 'Target 3']
missing_req_cols = [col for col in required_cols if col not in df.columns]
if missing_req_cols:
    raise ValueError(f"DataFrame 'df' is missing required columns: {missing_req_cols}")

print("--- Initial Data Sample (from df) ---")
print(df.head())
print(f"\nInitial DataFrame shape: {df.shape}")
print("\nInitial Null Counts:\n", df.isnull().sum().sort_values(ascending=False).head()) # Show top nulls
print("\nInitial Data Types:\n", df.dtypes.value_counts())

# --- Preprocessing ---

# Step 1: Clean column names (strip whitespace)
print("\n--- 1. Cleaning Column Names ---")
df.columns = df.columns.str.strip()
print("Cleaned columns list (first 10):", df.columns.tolist()[:10])

# Step 2: Convert object columns to numeric where possible (excluding 'Company')
print("\n--- 2. Converting Columns to Numeric (where possible) ---")
potential_numeric_cols = df.select_dtypes(include='object').columns
if 'Company' in potential_numeric_cols:
    potential_numeric_cols = potential_numeric_cols.drop('Company')

converted_cols_report = []
for col in potential_numeric_cols:
    # Check if column still exists (might be dropped if all NaN later)
    if col not in df.columns: continue
    original_non_nulls = df[col].notnull().sum()
    if original_non_nulls == 0: continue # Skip if all null

    # Attempt conversion
    try:
        converted_col = df[col].astype(str).str.replace(',', '', regex=False)
        converted_col = pd.to_numeric(converted_col, errors='coerce')

        if pd.api.types.is_numeric_dtype(converted_col) and converted_col.notnull().sum() > 0:
             df[col] = converted_col
             converted_cols_report.append(f"'{col}'") # ({(converted_col.notnull().sum() / original_non_nulls * 100):.1f}% numeric)")
    except Exception as e:
        print(f"Could not convert column '{col}': {e}")


if converted_cols_report:
    print(f"Successfully converted to numeric: {', '.join(converted_cols_report)}")
else:
    print("No object columns (excluding 'Company') were converted to numeric.")

# Step 3: Separate Columns and Identify Features for Processing
print("\n--- 3. Separating Columns & Identifying Processing Features ---")
target_cols = ['Target 1', 'Target 2', 'Target 3']

# Preserve essential columns before processing others
company_col_preserved = df['Company'].copy()
year_col_preserved = df['Year'].copy()
target_df_preserved = df[target_cols].copy()

# Identify numeric columns intended for imputation/scaling/capping
# Exclude Year, Company_ID (if exists from sample), and Targets
numeric_processing_candidates = df.select_dtypes(include=np.number).columns
cols_to_exclude = target_cols + ['Year', 'Company_ID'] # Exclude Company_ID from sample if present
numeric_processing_cols = numeric_processing_candidates.drop(
    cols_to_exclude, errors='ignore'
).tolist()

# --- Sanity Checks ---
if 'Year' in numeric_processing_cols: raise ValueError("FATAL ERROR: 'Year' was incorrectly included for processing!")
if 'Company' in numeric_processing_cols: raise ValueError("FATAL ERROR: 'Company' name included for processing!")
if any(tc in numeric_processing_cols for tc in target_cols): raise ValueError("FATAL ERROR: Target columns included for processing!")
# --- End Sanity Checks ---

numeric_df_to_process = df[numeric_processing_cols].copy()
print(f"{len(numeric_processing_cols)} numeric columns identified for processing (e.g., {numeric_processing_cols[:5]}...)")
if not numeric_processing_cols:
    print("Warning: No numeric columns found for processing (excluding Year/Targets). LSTM input will be empty.")


# Step 4: KNN Imputation
print("\n--- 4. Performing KNN Imputation ---")
if not numeric_df_to_process.empty and numeric_df_to_process.isnull().any().any():
    null_counts_before = numeric_df_to_process.isnull().sum()
    print(f"Nulls before imputation in {null_counts_before[null_counts_before > 0].count()} columns (Total: {null_counts_before.sum()})")
    # print(f"Example columns with nulls:\n{null_counts_before[null_counts_before > 0].head()}")

    imputer = KNNImputer(n_neighbors=N_NEIGHBORS_IMPUTE)
    # Important: Ensure data is float for KNNImputer
    numeric_df_to_process = numeric_df_to_process.astype(np.float32)

    imputed_array = imputer.fit_transform(numeric_df_to_process)
    numeric_df_processed = pd.DataFrame(imputed_array, columns=numeric_processing_cols, index=numeric_df_to_process.index)

    null_counts_after = numeric_df_processed.isnull().sum()
    if null_counts_after.sum() > 0:
        print(f"WARNING: Nulls remain after KNN imputation in {null_counts_after[null_counts_after > 0].count()} columns!")
    else:
        print("Imputation complete. No remaining nulls in processed columns.")
else:
    print("No missing values found in numeric processing columns or no columns to process.")
    numeric_df_processed = numeric_df_to_process # Keep original if no imputation needed

# Step 5: Robust Scaling
print("\n--- 5. Applying Robust Scaler ---")
cols_to_scale = numeric_processing_cols

if cols_to_scale and not numeric_df_processed.empty:
    scaler = RobustScaler()
    scaled_data = scaler.fit_transform(numeric_df_processed[cols_to_scale])
    numeric_df_scaled = pd.DataFrame(scaled_data, columns=cols_to_scale, index=numeric_df_processed.index)
    print(f"Scaled {len(cols_to_scale)} columns using RobustScaler.")
else:
    print("No columns identified or available for scaling.")
    numeric_df_scaled = numeric_df_processed

# Step 6: Capping Outliers (on Scaled Data)
print("\n--- 6. Capping Outliers (on Scaled Data) ---")
if cols_to_scale and not numeric_df_scaled.empty:
    numeric_df_capped = numeric_df_scaled.copy()
    capped_cols_count = 0
    for col in cols_to_scale:
        col_data = numeric_df_capped[col].dropna()
        if not col_data.empty:
            col_mean = col_data.mean()
            col_std = col_data.std()
            if pd.notna(col_std) and col_std > 1e-6: # Check std dev is valid and non-zero
                lower_bound = col_mean - Z_SCORE_CAP * col_std
                upper_bound = col_mean + Z_SCORE_CAP * col_std
                original_min = numeric_df_capped[col].min() # Check if capping occurs
                original_max = numeric_df_capped[col].max()

                numeric_df_capped[col] = numeric_df_capped[col].clip(lower_bound, upper_bound)

                if numeric_df_capped[col].min() > original_min or numeric_df_capped[col].max() < original_max:
                     capped_cols_count += 1
    if capped_cols_count > 0:
         print(f"Capped outliers (>{Z_SCORE_CAP} std dev on scaled data) in {capped_cols_count} columns.")
    else:
         print("No outliers needed capping based on the Z-score threshold.")
    numeric_df_final_processed = numeric_df_capped
else:
    print("No columns available for capping.")
    numeric_df_final_processed = numeric_df_scaled


# Step 7: Reconstruct Final DataFrame for Sequence Generation
print("\n--- 7. Reconstructing Final DataFrame ---")
# Ensure indices align before concatenating
final_df = pd.concat([
    company_col_preserved.reset_index(drop=True),
    year_col_preserved.reset_index(drop=True),        # Original Year
    numeric_df_final_processed.reset_index(drop=True), # Processed numeric features
    target_df_preserved.reset_index(drop=True)        # Original Targets
], axis=1)

# Step 8: Label Encode Company (Important for Embedding Layer)
print("\n--- 8. Encoding Company Names ---")
le = LabelEncoder()
# Use the original 'Company' column for consistent encoding
final_df['Company_ID_Encoded'] = le.fit_transform(final_df['Company'])
num_companies = final_df['Company_ID_Encoded'].nunique()
print(f"Encoded 'Company' into 'Company_ID_Encoded'. Found {num_companies} unique companies.")
company_mapping = dict(zip(le.classes_, le.transform(le.classes_)))


print("\n--- Final Preprocessed Data Sample (Ready for Sequence Generation) ---")
print(final_df.head())
print("\nColumns in final_df:", final_df.columns.tolist())
print(f"\nFinal DataFrame shape: {final_df.shape}")
# Final check for NaNs in the columns used for LSTM features + Targets
check_cols_final = numeric_processing_cols + target_cols
print(f"\nFinal Null Counts in LSTM feature/target columns:\n{final_df[check_cols_final].isnull().sum().sort_values(ascending=False).head()}")
if final_df[check_cols_final].isnull().any().any():
      print("\nWARNING: NaNs detected in final columns used for LSTM input/targets. This will likely cause errors during training.")


# --- LSTM Data Preparation ---

# Step 9: Sort Data (Crucial for time series sequence generation)
print("\n--- 9. Sorting Data for Sequence Generation ---")
# Use the NEWLY CREATED Company_ID_Encoded for sorting
final_df = final_df.sort_values(by=['Company_ID_Encoded', 'Year']).reset_index(drop=True)

# Step 10: Define Features and Targets for LSTM
print("\n--- 10. Defining LSTM Features and Targets ---")
# Features for LSTM sequence are the processed numeric columns
numeric_features_for_lstm = numeric_processing_cols # These are already imputed/scaled/capped

print(f"Features for LSTM sequence input ({len(numeric_features_for_lstm)}): {numeric_features_for_lstm[:5]}...") # Print first few
print(f"Target columns ({len(target_cols)}): {target_cols}")
if not numeric_features_for_lstm:
     print("WARNING: No features identified for LSTM input. Model will likely fail.")


# Step 11: Sequence Creation Function (Includes NaN checks within)
print("\n--- 11. Defining Sequence Creation Function ---")
def create_sequences(df, numeric_cols, target_cols, company_col_id_encoded, year_col, min_hist_years=1):
    """Creates sequences, checks for NaNs in inputs/targets."""
    all_sequences = []
    all_company_ids = []
    all_targets = []
    max_len = 0
    skipped_nan_sequences = 0
    skipped_nan_targets = 0

    print(f"Generating sequences with min history of {min_hist_years} years...")
    company_groups = df.groupby(company_col_id_encoded)
    num_groups = len(company_groups)

    for company_id, group in company_groups:
        group = group.sort_values(year_col)
        features = group[numeric_cols].values
        targets = group[target_cols].values

        for i in range(min_hist_years, len(group)):
            current_sequence = features[:i, :]
            current_target = targets[i, :]

            # --- NaN Check ---
            if np.isnan(current_sequence).any() or np.isinf(current_sequence).any():
                 skipped_nan_sequences += 1
                 continue # Skip sequence if it contains NaN/Inf
            if np.isnan(current_target).any() or np.isinf(current_target).any():
                 skipped_nan_targets += 1
                 continue # Skip if target is NaN/Inf
            # --- End NaN Check ---

            all_sequences.append(current_sequence)
            all_company_ids.append(company_id)
            all_targets.append(current_target)
            if current_sequence.shape[0] > max_len:
                max_len = current_sequence.shape[0]

    if skipped_nan_sequences > 0: print(f"Warning: Skipped {skipped_nan_sequences} sequences due to NaN/Inf in features.")
    if skipped_nan_targets > 0: print(f"Warning: Skipped {skipped_nan_targets} sequences due to NaN/Inf in targets.")

    if not all_sequences:
        print("Warning: No valid (non-NaN) sequences generated.")
        return np.array([]), np.array([]), np.array([]), 0

    print(f"Generated {len(all_sequences)} valid sequences. Max sequence length: {max_len}")

    X_padded = pad_sequences(all_sequences, maxlen=max_len, dtype='float32', padding='pre', truncating='pre')
    y_array = np.array(all_targets, dtype='float32')
    X_comp_array = np.array(all_company_ids, dtype='int32')

    # Final check on generated arrays
    if np.isnan(X_padded).any(): print("FATAL WARNING: NaNs found in final X_padded sequence array!")
    if np.isnan(y_array).any(): print("FATAL WARNING: NaNs found in final y_array target array!")


    return X_padded, X_comp_array, y_array, max_len

# --- Model Definition ---
print("\n--- 12. Defining LSTM Model Architecture ---")
# (Using the same build_lstm_model function from previous answers)
def build_lstm_model(max_sequence_length, num_numeric_features, num_companies, num_targets,
                     embedding_dim=EMBEDDING_DIM, lstm_units=LSTM_UNITS, dense_units=DENSE_UNITS):
    sequence_input = Input(shape=(max_sequence_length, num_numeric_features), name='Sequence_Input')
    company_input = Input(shape=(1,), name='Company_Input')
    company_emb = Embedding(input_dim=num_companies, output_dim=embedding_dim,
                            embeddings_initializer='he_normal', name='Company_Embedding')(company_input)
    company_emb_flat = Flatten(name='Flatten_Embedding')(company_emb)
    masked_input = Masking(mask_value=0.0)(sequence_input)
    lstm_out = LSTM(lstm_units, kernel_initializer='he_normal', name='LSTM_Layer')(masked_input)
    merged = Concatenate(name='Concatenate_LSTM_Embedding')([lstm_out, company_emb_flat])
    x = Dense(dense_units, activation='relu', kernel_initializer='he_normal', name='Dense_1')(merged)
    output = Dense(num_targets, name='Output_Layer')(x)
    model = Model(inputs=[sequence_input, company_input], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Adjusted learning rate
                  loss='mse',
                  metrics=[RootMeanSquaredError(name='rmse')])
    return model

# --- Custom Training Loop (Expanding Window with NaN Checks) ---
print("\n--- 13. Starting Expanding Window Training ---")
results = {}
unique_years = sorted(final_df['Year'].unique())
print(f"Unique years found in data for training loop: {unique_years}")

if len(unique_years) <= MIN_HISTORY_YEARS:
    print(f"Not enough distinct years ({len(unique_years)}) for training (min history {MIN_HISTORY_YEARS}).")
else:
    for test_year_index in range(MIN_HISTORY_YEARS, len(unique_years)):
        test_year = unique_years[test_year_index]
        train_end_year = unique_years[test_year_index - 1]

        print(f"\n{'='*15} Training for Prediction of Year: {int(test_year)} {'='*15}")
        print(f"Using training data UP TO year: {int(train_end_year)}")

        # 1. Filter training data
        train_df = final_df[final_df['Year'] <= train_end_year].copy()

        # 2. Create training sequences (Function now includes NaN checks)
        X_seq_train, X_cid_train, y_train, max_seq_len_train = create_sequences(
            train_df, numeric_features_for_lstm, target_cols, 'Company_ID_Encoded', 'Year',
            min_hist_years=MIN_HISTORY_YEARS
        )

        if X_seq_train.shape[0] == 0 or max_seq_len_train == 0:
             print(f"No valid training sequences generated. Skipping fold.")
             continue

        # 3. Prepare test sequences (with NaN checks during creation)
        print(f"\nPreparing test sequences to predict year {int(test_year)}...")
        test_sequences = []
        test_company_ids = []
        test_actuals = []
        skipped_test_nan_features = 0
        skipped_test_nan_targets = 0

        test_candidate_df = final_df[final_df['Year'] <= test_year].copy()
        companies_in_test_year = 0
        sequences_for_test = 0

        for company_id, group in test_candidate_df.groupby('Company_ID_Encoded'):
            if test_year in group['Year'].values:
                 companies_in_test_year += 1
                 group_hist = group[group['Year'] <= train_end_year]
                 if not group_hist.empty and len(group_hist) >= MIN_HISTORY_YEARS:
                     current_sequence = group_hist[numeric_features_for_lstm].values
                     actual_targets = group[group['Year'] == test_year][target_cols].values.flatten()

                     # --- Test Data NaN Checks ---
                     if np.isnan(current_sequence).any() or np.isinf(current_sequence).any():
                          skipped_test_nan_features += 1
                          continue # Skip this test sample if features have NaN/Inf
                     if np.isnan(actual_targets).any() or np.isinf(actual_targets).any():
                          skipped_test_nan_targets += 1
                          continue # Skip this test sample if targets have NaN/Inf
                     # --- End Test Data NaN Checks ---

                     test_sequences.append(current_sequence)
                     test_company_ids.append(company_id)
                     test_actuals.append(actual_targets)
                     sequences_for_test += 1

        if skipped_test_nan_features > 0: print(f"Warning: Skipped {skipped_test_nan_features} test sequences due to NaN/Inf in features.")
        if skipped_test_nan_targets > 0: print(f"Warning: Skipped {skipped_test_nan_targets} test sequences due to NaN/Inf in targets.")

        if not test_sequences:
            print(f"No valid test sequences generated after NaN checks. Skipping prediction phase.")
            continue

        X_seq_test = pad_sequences(test_sequences, maxlen=max_seq_len_train, dtype='float32', padding='pre', truncating='pre')
        X_cid_test = np.array(test_company_ids, dtype='int32')
        y_test_actual = np.array(test_actuals, dtype='float32')

        print(f"Prepared {X_seq_test.shape[0]} valid test sequences for year {int(test_year)}.")
        # Final check on test arrays
        if np.isnan(X_seq_test).any(): print("FATAL WARNING: NaNs found in final X_seq_test array!")
        if np.isnan(y_test_actual).any(): print("FATAL WARNING: NaNs found in final y_test_actual array!")

        # 4. Build model
        print("\nBuilding/Re-initializing model...")
        model = build_lstm_model(max_seq_len_train, len(numeric_features_for_lstm), num_companies, len(target_cols))
        # Optional: Print summary once
        # if test_year_index == MIN_HISTORY_YEARS: model.summary()

        # 5. Train model
        print(f"\nTraining model on {X_seq_train.shape[0]} sequences...")
        early_stopping = EarlyStopping(monitor='val_rmse', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=1)

        history = model.fit(
            [X_seq_train, X_cid_train.reshape(-1, 1)],
            y_train,
            epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,
            callbacks=[early_stopping], verbose=0
        )
        epochs_trained = len(history.history['loss'])
        print(f"Training complete. Trained for {epochs_trained} epochs.")
        best_val_rmse = min(history.history.get('val_rmse', [np.inf])) # Handle case where validation might not run
        print(f"Best Validation RMSE achieved: {best_val_rmse:.4f}") # Might be NaN if validation data had issues or training failed instantly

        # 6. Predict
        print(f"\nPredicting for year {int(test_year)}...")
        predictions = model.predict([X_seq_test, X_cid_test.reshape(-1, 1)])

        # Check predictions for NaNs
        if np.isnan(predictions).any() or np.isinf(predictions).any():
             print("WARNING: Model produced NaN/Inf predictions! Evaluation will be NaN.")
             # Optionally, save problematic inputs/outputs for debugging

        # 7. Store results
        results[test_year] = {'company_ids': X_cid_test, 'predictions': predictions, 'actuals': y_test_actual}
        print(f"Stored predictions and actuals for {int(test_year)}.")

        # 8. Evaluate fold performance
        print(f"\n--- Test Year {int(test_year)} Performance ---")
        # Check inputs before calculating RMSE
        if np.isnan(predictions).any() or np.isinf(predictions).any() or np.isnan(y_test_actual).any() or np.isinf(y_test_actual).any():
             fold_rmse = np.nan
             print("Cannot calculate Test RMSE due to NaN/Inf in predictions or actuals.")
        else:
             fold_mse = np.mean((predictions - y_test_actual)**2)
             fold_rmse = np.sqrt(fold_mse)
             if np.isnan(fold_rmse): fold_rmse = np.nan # Ensure it's NaN if sqrt fails

        print(f"Overall Test RMSE for this fold: {fold_rmse:.4f}")


# --- Post-Loop Analysis ---
print(f"\n{'='*20} Training Loop Finished {'='*20}")
# (Using the same post-loop analysis code from the previous answer to calculate overall RMSE excluding NaN folds)
all_actuals_list = []
all_predictions_list = []
nan_folds_report = []
if results:
    print("\nCalculating Overall Performance Across All Test Years:")
    for year in sorted(results.keys()):
        data = results[year]
        # Check for NaNs before aggregating
        if np.isnan(data['predictions']).any() or np.isinf(data['predictions']).any() or \
           np.isnan(data['actuals']).any() or np.isinf(data['actuals']).any():
            nan_folds_report.append(int(year))
            print(f" - Excluding results from test year {int(year)} due to NaN/Inf values.")
        else:
            all_actuals_list.append(data['actuals'])
            all_predictions_list.append(data['predictions'])
            # print(f" - Included results from test year: {int(year)} ({data['predictions'].shape[0]} predictions)")

    if all_actuals_list:
        all_actuals_np = np.concatenate(all_actuals_list, axis=0)
        all_predictions_np = np.concatenate(all_predictions_list, axis=0)

        final_mse = np.mean((all_predictions_np - all_actuals_np)**2)
        final_rmse = np.sqrt(final_mse)
        final_mae = np.mean(np.abs(all_predictions_np - all_actuals_np)) # Also calculate MAE

        print(f"\nFinal Overall RMSE (excluding {len(nan_folds_report)} NaN folds): {final_rmse:.4f}")
        print(f"Final Overall MAE (excluding {len(nan_folds_report)} NaN folds): {final_mae:.4f}")
        if nan_folds_report:
            print(f"NaN RMSE occurred in folds for test years: {nan_folds_report}")
    else:
         print("No valid (non-NaN) predictions were generated across any folds to calculate overall performance.")
else:
    print("No results were generated during the training loop.")

print("\n--- Script Complete ---")

df

"""# LSTM"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA # Keep PCA
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding,
                                     Concatenate, Masking, Flatten, Dropout)
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
import gc

# --- Configuration ---
# Preprocessing
N_NEIGHBORS_IMPUTE = 5
Z_SCORE_CAP = 3
PCA_VARIANCE_RATIO = 0.95 # Keep PCA variance threshold
# Model Hyperparameters
EMBEDDING_DIM = 16
LSTM_UNITS_L1 = 128
LSTM_UNITS_L2 = 64
DENSE_UNITS = 64
DROPOUT_RATE = 0.25
LEARNING_RATE = 1e-4
# Training Parameters
EPOCHS = 75
BATCH_SIZE = 32
VALIDATION_SPLIT = 0.15
EARLY_STOPPING_PATIENCE = 12
REDUCE_LR_PATIENCE = 6
MIN_HISTORY_YEARS = 2

warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
tf.get_logger().setLevel('WARN')
tf.autograph.set_verbosity(0)

# --- Load Data ---
# Assume df is loaded
# df = pd.read_csv('your_data.csv')
print("--- Initial Data Sample (from df) ---")
print(df.head())
print(f"\nInitial DataFrame shape: {df.shape}")

# --- Preprocessing ---

# 1. Clean column names
print("\n--- 1. Cleaning Column Names ---")
df.columns = df.columns.str.strip()

# 2. Convert object columns to numeric
print("\n--- 2. Converting Columns to Numeric ---")
# (Assuming this part works as intended)
potential_numeric_cols = df.select_dtypes(include='object').columns
if 'Company' in potential_numeric_cols: potential_numeric_cols = potential_numeric_cols.drop('Company')
converted_count = 0
for col in potential_numeric_cols:
    if col not in df.columns: continue
    try:
        original_type = df[col].dtype
        converted_col = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce')
        if pd.api.types.is_numeric_dtype(converted_col) and converted_col.notnull().sum() > 0:
             df[col] = converted_col
             if df[col].dtype != original_type: converted_count += 1
    except Exception: pass
print(f"Attempted numeric conversion. {converted_count} columns changed type.")

# --- [MODIFIED] Step 3: Feature Engineering (Generic Differences) ---
print("\n--- 3. Feature Engineering (Generic Differences) ---")
target_cols = ['Target 1', 'Target 2', 'Target 3']
engineered_features = []

# Identify original numeric features BEFORE creating differences
original_numeric_cols = df.select_dtypes(include=np.number).columns.drop(
    target_cols + ['Year', 'Company_ID'], errors='ignore' # Exclude targets, Year, and maybe old Company_ID
).tolist()

print(f"Calculating YoY differences for {len(original_numeric_cols)} original numeric features...")
df = df.sort_values(by=['Company', 'Year']) # CRITICAL: Sort before using shift/diff

for col in original_numeric_cols:
    # Ensure the original column is numeric first
    df[col] = pd.to_numeric(df[col], errors='coerce')
    # Calculate absolute difference
    diff_col_name = f'FE_{col}_Diff'
    df[diff_col_name] = df.groupby('Company')[col].diff()
    # Convert to float32 and add to list for processing
    df[diff_col_name] = df[diff_col_name].astype(np.float32)
    engineered_features.append(diff_col_name)

print(f" - Created {len(engineered_features)} absolute difference (YoY) features.")

# --- [MODIFIED] Step 4: Separate Columns & Identify ALL Features for Processing ---
print("\n--- 4. Separating Columns & Identifying Processing Features ---")
company_col_preserved = df['Company'].copy()
year_col_preserved = df['Year'].copy()
target_df_preserved = df[target_cols].copy() # Preserve original targets

# Start with the original numeric cols + newly engineered diff cols
numeric_processing_cols = original_numeric_cols + engineered_features

# Remove any duplicates just in case, and ensure they exist
numeric_processing_cols = sorted(list(set(
    col for col in numeric_processing_cols if col in df.columns
)))

# --- Final Sanity Checks ---
if 'Year' in numeric_processing_cols: raise ValueError("FATAL: 'Year' included for processing!")
if 'Company' in numeric_processing_cols: raise ValueError("FATAL: 'Company' included for processing!")
if any(tc in numeric_processing_cols for tc in target_cols): raise ValueError("FATAL: Targets included for processing!")
# --- End Sanity Checks ---

numeric_df_to_process = df[numeric_processing_cols].copy()
print(f"{len(numeric_processing_cols)} numeric columns identified for processing (Original + Engineered Diffs).")

# 5. KNN Imputation (Features + Engineered Features)
print("\n--- 5. Performing KNN Imputation ---")
# (Impute the combined set of original features and difference features)
if not numeric_df_to_process.empty and numeric_df_to_process.isnull().any().any():
    null_counts_before = numeric_df_to_process.isnull().sum()
    print(f"Nulls before imputation in {null_counts_before[null_counts_before > 0].count()} columns (Total: {null_counts_before.sum()})")
    imputer = KNNImputer(n_neighbors=N_NEIGHBORS_IMPUTE)
    numeric_df_to_process = numeric_df_to_process.astype(np.float32)
    imputed_array = imputer.fit_transform(numeric_df_to_process)
    numeric_df_processed = pd.DataFrame(imputed_array, columns=numeric_processing_cols, index=numeric_df_to_process.index)
    if numeric_df_processed.isnull().sum().sum() > 0: print("WARNING: Nulls remain after KNN imputation!")
    else: print("Imputation complete.")
else:
    print("No missing values found in processing columns or no columns to process.")
    numeric_df_processed = numeric_df_to_process

# 6. Robust Scaling (Features + Engineered Features)
print("\n--- 6. Applying Robust Scaler ---")
cols_to_scale = numeric_processing_cols
if cols_to_scale and not numeric_df_processed.empty:
    feature_scaler = RobustScaler()
    scaled_data = feature_scaler.fit_transform(numeric_df_processed[cols_to_scale])
    numeric_df_scaled = pd.DataFrame(scaled_data, columns=cols_to_scale, index=numeric_df_processed.index)
    print(f"Scaled {len(cols_to_scale)} columns (Original + Diffs) using RobustScaler.")
else:
    print("No columns identified or available for scaling.")
    numeric_df_scaled = numeric_df_processed

# 7. Capping Outliers (on Scaled Features + Engineered Features)
print("\n--- 7. Capping Outliers ---")
# (Cap the combined set)
if cols_to_scale and not numeric_df_scaled.empty:
    numeric_df_capped = numeric_df_scaled.copy()
    capped_cols_count = 0
    for col in cols_to_scale:
        col_data = numeric_df_capped[col].dropna();
        if not col_data.empty:
            col_mean=col_data.mean(); col_std=col_data.std()
            if pd.notna(col_std) and col_std > 1e-6:
                lower=col_mean-Z_SCORE_CAP*col_std; upper=col_mean+Z_SCORE_CAP*col_std
                min_orig=numeric_df_capped[col].min(); max_orig=numeric_df_capped[col].max()
                numeric_df_capped[col]=numeric_df_capped[col].clip(lower, upper)
                if numeric_df_capped[col].min()>min_orig or numeric_df_capped[col].max()<max_orig: capped_cols_count+=1
    if capped_cols_count > 0: print(f"Capped outliers in {capped_cols_count} columns.")
    else: print("No outliers needed capping.")
    numeric_df_final_processed = numeric_df_capped
else:
    print("No columns available for capping.")
    numeric_df_final_processed = numeric_df_scaled

# 8. Apply PCA (on combined imputed, scaled, capped features)
print(f"\n--- 8. Applying PCA (Retaining {PCA_VARIANCE_RATIO*100:.0f}% Variance) ---")
n_original_features = numeric_df_final_processed.shape[1]
if n_original_features > 0:
    pca = PCA(n_components=PCA_VARIANCE_RATIO)
    pca_components = pca.fit_transform(numeric_df_final_processed)
    n_components = pca.n_components_
    print(f"PCA applied. Reduced features from {n_original_features} to {n_components}.")
    pc_cols = [f'PC_{i+1}' for i in range(n_components)]
    pca_df = pd.DataFrame(pca_components, columns=pc_cols, index=numeric_df_final_processed.index)
    numeric_features_for_lstm = pc_cols # LSTM will use these PC columns
else:
    print("Skipping PCA: No numeric features to process.")
    pca_df = pd.DataFrame(index=df.index)
    numeric_features_for_lstm = []
    n_components = 0

# 9. Reconstruct DataFrame (Using PCA Components)
print("\n--- 9. Reconstructing DataFrame with PCA Components ---")
final_df = pd.concat([
    company_col_preserved.reset_index(drop=True),
    year_col_preserved.reset_index(drop=True),
    pca_df.reset_index(drop=True),             # PCA components
    target_df_preserved.reset_index(drop=True) # Original Targets
], axis=1)

# 10. Scale Target Variables
print("\n--- 10. Scaling Target Variables (Separately) ---")
# (Same code as before to scale targets and store scalers)
target_scalers = {}
final_df_scaled_targets = final_df.copy()
for target_col in target_cols:
    if target_col not in final_df_scaled_targets.columns: continue
    scaler = StandardScaler()
    target_data = final_df_scaled_targets[[target_col]].astype(np.float32)
    valid_target_data = target_data.dropna()
    if not valid_target_data.empty:
        scaler.fit(valid_target_data)
        final_df_scaled_targets[target_col] = scaler.transform(target_data)
        target_scalers[target_col] = scaler
        print(f"Applied StandardScaler to target column: '{target_col}'")
    else: target_scalers[target_col] = None

# 11. Label Encode Company
print("\n--- 11. Encoding Company Names ---")
le = LabelEncoder()
final_df_scaled_targets['Company_ID_Encoded'] = le.fit_transform(final_df_scaled_targets['Company'])
num_companies = final_df_scaled_targets['Company_ID_Encoded'].nunique()
print(f"Encoded 'Company' into 'Company_ID_Encoded'. Found {num_companies} unique companies.")

print("\n--- Final Preprocessed Data Sample (PCA Features, Scaled Targets) ---")
print(final_df_scaled_targets.head())
print(f"\nFinal DataFrame shape: {final_df_scaled_targets.shape}")
# Final check for NaNs in PCA components
if not numeric_features_for_lstm: print("No PCA features to check.")
elif final_df_scaled_targets[numeric_features_for_lstm].isnull().any().any():
      print(f"\nWARNING: NaNs detected in final PCA columns:\n{final_df_scaled_targets[numeric_features_for_lstm].isnull().sum().sort_values(ascending=False).head()}")

# --- LSTM Data Preparation ---

# 12. Sort Data
print("\n--- 12. Sorting Data ---")
final_df_scaled_targets = final_df_scaled_targets.sort_values(by=['Company_ID_Encoded', 'Year']).reset_index(drop=True)

# 13. Define LSTM Features (Already defined as numeric_features_for_lstm = pc_cols)
print("\n--- 13. Defining LSTM Features (Using PCA Components) ---")
print(f"Features for LSTM sequence input ({len(numeric_features_for_lstm)}): {numeric_features_for_lstm[:5]}...")

# 14. Sequence Creation Function
print("\n--- 14. Defining Sequence Creation Function ---")
# (Function definition remains the same)
def create_sequences(df, numeric_cols, target_cols, company_col_id_encoded, year_col, min_hist_years=1):
    # ... (exact same function implementation) ...
    all_sequences, all_company_ids, all_targets = [], [], []
    max_len = 0; skipped_nan_sequences, skipped_nan_targets = 0, 0
    company_groups = df.groupby(company_col_id_encoded)
    for company_id, group in company_groups:
        group = group.sort_values(year_col); features = group[numeric_cols].values; targets = group[target_cols].values
        for i in range(min_hist_years, len(group)):
            current_sequence = features[:i, :]; current_target = targets[i, :] # Get all 3 scaled targets
            if np.isnan(current_sequence).any() or np.isinf(current_sequence).any(): skipped_nan_sequences += 1; continue
            if np.isnan(current_target).any() or np.isinf(current_target).any(): skipped_nan_targets += 1; continue
            all_sequences.append(current_sequence); all_company_ids.append(company_id); all_targets.append(current_target)
            if current_sequence.shape[0] > max_len: max_len = current_sequence.shape[0]
    if skipped_nan_sequences > 0: print(f"Warning: Skipped {skipped_nan_sequences} sequences (NaN features).")
    if skipped_nan_targets > 0: print(f"Warning: Skipped {skipped_nan_targets} sequences (NaN targets).")
    if not all_sequences: return np.array([]), np.array([]), np.array([]), 0
    X_padded = pad_sequences(all_sequences, maxlen=max_len, dtype='float32', padding='pre', truncating='pre')
    y_array = np.array(all_targets, dtype='float32'); X_comp_array = np.array(all_company_ids, dtype='int32')
    if np.isnan(X_padded).any(): print("FATAL WARNING: NaNs in X_padded!")
    if np.isnan(y_array).any(): print("FATAL WARNING: NaNs in y_array!")
    return X_padded, X_comp_array, y_array, max_len

# 15. Define Model Architecture Function (Input size now depends on n_components)
print("\n--- 15. Defining LSTM Model Architecture Function ---")
# (Definition remains the same - stacked LSTM with dropout)
def build_lstm_model(max_sequence_length, num_numeric_features, num_companies, num_targets_out,
                     embedding_dim=EMBEDDING_DIM, lstm_units_l1=LSTM_UNITS_L1, lstm_units_l2=LSTM_UNITS_L2,
                     dense_units=DENSE_UNITS, dropout_rate=DROPOUT_RATE, learning_rate=LEARNING_RATE):
    # ... (Exact same stacked LSTM model definition as previous answer) ...
    sequence_input = Input(shape=(max_sequence_length, num_numeric_features), name='Sequence_Input')
    company_input = Input(shape=(1,), name='Company_Input')
    company_emb = Embedding(input_dim=num_companies, output_dim=embedding_dim,
                            embeddings_initializer='he_normal', name='Company_Embedding')(company_input)
    company_emb_flat = Flatten(name='Flatten_Embedding')(company_emb)
    masked_input = Masking(mask_value=0.0)(sequence_input)
    lstm_layer1 = LSTM(lstm_units_l1, kernel_initializer='he_normal', return_sequences=True, name='LSTM_Layer_1')(masked_input)
    lstm_layer1 = Dropout(dropout_rate)(lstm_layer1)
    lstm_layer2 = LSTM(lstm_units_l2, kernel_initializer='he_normal', return_sequences=False, name='LSTM_Layer_2')(lstm_layer1)
    lstm_layer2 = Dropout(dropout_rate)(lstm_layer2)
    merged = Concatenate(name='Concatenate_LSTM_Embedding')([lstm_layer2, company_emb_flat])
    x = Dense(dense_units, activation='relu', kernel_initializer='he_normal', name='Dense_1')(merged)
    x = Dropout(dropout_rate)(x)
    output = Dense(num_targets_out, name='Output_Layer')(x) # num_targets_out will be 1
    model = Model(inputs=[sequence_input, company_input], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='mse',
                  metrics=[RootMeanSquaredError(name='rmse')])
    return model


# --- Custom Training Loop (Input features are PCs) ---
print("\n--- 16. Starting Expanding Window Training (PCA Features) ---")
results = {}
unique_years = sorted(final_df_scaled_targets['Year'].unique())
print(f"Unique years found: {unique_years}")

if len(unique_years) <= MIN_HISTORY_YEARS:
    print(f"Not enough distinct years ({len(unique_years)}) for training.")
else:
    # Outer loop: Iterate through years
    for test_year_index in range(MIN_HISTORY_YEARS, len(unique_years)):
        test_year = unique_years[test_year_index]; train_end_year = unique_years[test_year_index - 1]
        print(f"\n{'='*10} Processing Fold for Test Year: {int(test_year)} (Train up to {int(train_end_year)}) {'='*10}")

        # 1. Filter & Create base sequences (using PCA columns)
        train_df = final_df_scaled_targets[final_df_scaled_targets['Year'] <= train_end_year].copy()
        # Pass the PCA feature names to the sequence creator
        X_seq_train, X_cid_train, y_train_scaled_all, max_seq_len_train = create_sequences(
            train_df, numeric_features_for_lstm, target_cols, 'Company_ID_Encoded', 'Year', min_hist_years=MIN_HISTORY_YEARS)
        if X_seq_train.shape[0] == 0 or max_seq_len_train == 0: print(f"No valid training sequences generated. Skipping fold."); continue
        print(f"Generated {X_seq_train.shape[0]} base training sequences (Input: {X_seq_train.shape[-1]} PCs, Max Len: {max_seq_len_train}).")

        # 2. Prepare base test sequences (using PCA columns)
        test_sequences, test_company_ids, test_actuals_scaled_all = [], [], []
        # ... (same NaN-checking loop as before, ensuring PCA features are used) ...
        test_candidate_df=final_df_scaled_targets[final_df_scaled_targets['Year']<=test_year].copy()
        for company_id, group in test_candidate_df.groupby('Company_ID_Encoded'):
            if test_year in group['Year'].values:
                 group_hist=group[group['Year']<=train_end_year]
                 if not group_hist.empty and len(group_hist)>=MIN_HISTORY_YEARS:
                     current_sequence=group_hist[numeric_features_for_lstm].values # Use PCA feature names
                     actual_targets_scaled=group[group['Year']==test_year][target_cols].values.flatten()
                     if np.isnan(current_sequence).any() or np.isinf(current_sequence).any(): continue
                     if np.isnan(actual_targets_scaled).any() or np.isinf(actual_targets_scaled).any(): continue
                     test_sequences.append(current_sequence); test_company_ids.append(company_id); test_actuals_scaled_all.append(actual_targets_scaled)
        if not test_sequences: print(f"No valid test sequences generated. Skipping prediction."); continue
        X_seq_test = pad_sequences(test_sequences, maxlen=max_seq_len_train, dtype='float32', padding='pre', truncating='pre')
        X_cid_test = np.array(test_company_ids, dtype='int32')
        y_test_actual_scaled_all = np.array(test_actuals_scaled_all, dtype='float32')
        print(f"Prepared {X_seq_test.shape[0]} base valid test sequences (Input: {X_seq_test.shape[-1]} PCs).")

        # --- Inner loop: Train separate model for each target ---
        results[test_year] = {}
        for target_idx, target_name in enumerate(target_cols):
            print(f"\n--- Training for Target: {target_name} (Year: {int(test_year)}) ---")

            y_train_target_scaled = y_train_scaled_all[:, target_idx]
            y_test_actual_target_scaled = y_test_actual_scaled_all[:, target_idx]
            if target_name not in target_scalers or target_scalers[target_name] is None: print(f"Scaler for {target_name} not found. Skipping."); continue

            # Build model (passing n_components)
            # Ensure n_components is defined from PCA step, handle case where PCA wasn't run
            current_n_features = n_components if numeric_features_for_lstm else 0
            if current_n_features == 0: print("No PCA features to train on. Skipping model."); continue

            model = build_lstm_model(max_seq_len_train, current_n_features, num_companies, 1) # Use n_components here!

            # Callbacks
            early_stopping = EarlyStopping(monitor='val_rmse', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=0)
            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.2, patience=REDUCE_LR_PATIENCE, min_lr=1e-6, verbose=0)

            # Train model
            print(f"Training model for {target_name}...")
            history = model.fit([X_seq_train, X_cid_train.reshape(-1, 1)], y_train_target_scaled,
                                epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,
                                callbacks=[early_stopping, reduce_lr], verbose=0)
            epochs_trained = len(history.history['loss']); best_val_rmse = min(history.history.get('val_rmse', [np.inf]))
            print(f"Training complete ({epochs_trained} epochs). Best Val RMSE (Scaled): {best_val_rmse:.4f}")

            # Predict (scaled)
            predictions_scaled = model.predict([X_seq_test, X_cid_test.reshape(-1, 1)], verbose=0)

            # Inverse Transform
            scaler_t = target_scalers[target_name]
            try:
                 predictions_original = scaler_t.inverse_transform(predictions_scaled).flatten()
                 y_test_actual_original = scaler_t.inverse_transform(y_test_actual_target_scaled.reshape(-1, 1)).flatten()
            except Exception as e:
                 print(f"ERROR during inverse transform for {target_name}: {e}. Storing NaNs.")
                 predictions_original = np.full(predictions_scaled.shape[0], np.nan)
                 y_test_actual_original = np.full(y_test_actual_target_scaled.shape[0], np.nan)

            # Store results
            results[test_year][target_name] = {'predictions_original': predictions_original, 'actuals_original': y_test_actual_original}

            # Evaluate fold performance (Original Scale)
            if np.isnan(predictions_original).any() or np.isnan(y_test_actual_original).any(): fold_rmse_orig = np.nan
            else: fold_rmse_orig = np.sqrt(np.mean((predictions_original - y_test_actual_original)**2))
            print(f"  --> Test RMSE for {target_name} (Original Scale): {fold_rmse_orig:.4f}")

            # Clean up memory
            del model; tf.keras.backend.clear_session(); gc.collect()

# --- Post-Loop Analysis (Separated per target) ---
print(f"\n{'='*20} Training Loop Finished {'='*20}")
# (Exact same post-loop analysis code as previous answer to print RMSE/MAE per target)
if results:
    print("\nCalculating Overall Performance Per Target (Original Scale):")
    for target_idx, target_name in enumerate(target_cols):
        print(f"\n--- Overall Performance for Target: {target_name} ---")
        all_actuals_orig_list = []; all_predictions_orig_list = []; nan_folds_report = []
        for year in sorted(results.keys()):
            if year in results and target_name in results[year]:
                data = results[year][target_name]; pred_orig = data['predictions_original']; act_orig = data['actuals_original']
                if np.isnan(pred_orig).any() or np.isinf(pred_orig).any() or np.isnan(act_orig).any() or np.isinf(act_orig).any():
                    nan_folds_report.append(int(year))
                else: all_actuals_orig_list.append(act_orig); all_predictions_orig_list.append(pred_orig)
        if all_actuals_orig_list:
            all_actuals_np = np.concatenate(all_actuals_orig_list, axis=0); all_predictions_np = np.concatenate(all_predictions_orig_list, axis=0)
            final_mse = np.mean((all_predictions_np - all_actuals_np)**2); final_rmse = np.sqrt(final_mse)
            final_mae = np.mean(np.abs(all_predictions_np - all_actuals_np))
            print(f"  Final Overall RMSE (Original Scale, excluding {len(nan_folds_report)} NaN folds): {final_rmse:.4f}")
            print(f"  Final Overall MAE (Original Scale, excluding {len(nan_folds_report)} NaN folds): {final_mae:.4f}")
            if nan_folds_report: print(f"  NaN results occurred for this target in test years: {nan_folds_report}")
        else: print(f"  No valid (non-NaN) predictions in original scale generated for {target_name}.")
else: print("No results generated.")

print("\n--- Script Complete ---")



"""# Encoder Decoder"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA # Keep PCA
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding,
                                     Concatenate, Masking, Flatten, Dropout,
                                     RepeatVector, TimeDistributed) # Added RepeatVector, TimeDistributed (though might simplify)
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
import gc

# --- Configuration ---
# Preprocessing
N_NEIGHBORS_IMPUTE = 5
Z_SCORE_CAP = 3
PCA_VARIANCE_RATIO = 0.95 # Keep PCA variance threshold
# Model Hyperparameters
EMBEDDING_DIM = 16
# --- Encoder/Decoder Specific ---
ENCODER_LSTM_UNITS = 128 # Units for the Encoder LSTM
# DECODER_LSTM_UNITS = 64 # Optional if using a recurrent decoder (see notes)
DENSE_UNITS = 64
DROPOUT_RATE = 0.25
LEARNING_RATE = 1e-4
# --- End Encoder/Decoder Specific ---
# Training Parameters
EPOCHS = 75
BATCH_SIZE = 32
VALIDATION_SPLIT = 0.15
EARLY_STOPPING_PATIENCE = 12
REDUCE_LR_PATIENCE = 6
MIN_HISTORY_YEARS = 2

warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
tf.get_logger().setLevel('WARN')
tf.autograph.set_verbosity(0)

# --- Load Data ---
# Assume df is loaded and has columns like 'Company', 'Year', numeric features,
# and 'Target 1', 'Target 2', 'Target 3'
# Create dummy data for demonstration if df is not loaded
if 'df' not in locals():
    print("--- Creating Dummy Data ---")
    n_companies = 20
    n_years = 10
    n_features = 15
    companies = [f'Comp_{i}' for i in range(n_companies)]
    years = list(range(2010, 2010 + n_years))
    data = []
    for comp in companies:
        comp_data = np.random.randn(n_years, n_features + 3) * 20 + 100 # Features + 3 targets
        # Introduce some NaNs
        nan_indices = np.random.choice(comp_data.size, size=int(comp_data.size * 0.05), replace=False)
        np.put(comp_data, nan_indices, np.nan)
        for i, year in enumerate(years):
            row = {'Company': comp, 'Year': year}
            row.update({f'Feature_{j+1}': comp_data[i, j] for j in range(n_features)})
            row.update({f'Target {k+1}': comp_data[i, n_features+k] for k in range(3)})
            data.append(row)
    df = pd.DataFrame(data)
    print("Dummy data created.")


print("--- Initial Data Sample (from df) ---")
print(df.head())
print(f"\nInitial DataFrame shape: {df.shape}")

# --- Preprocessing ---

# 1. Clean column names
print("\n--- 1. Cleaning Column Names ---")
df.columns = df.columns.str.strip()

# 2. Convert object columns to numeric
print("\n--- 2. Converting Columns to Numeric ---")
potential_numeric_cols = df.select_dtypes(include='object').columns
if 'Company' in potential_numeric_cols: potential_numeric_cols = potential_numeric_cols.drop('Company')
converted_count = 0
for col in potential_numeric_cols:
    if col not in df.columns: continue
    try:
        original_type = df[col].dtype
        # Handle potential non-string types before replacing
        col_str = df[col].astype(str)
        converted_col = pd.to_numeric(col_str.str.replace(',', '', regex=False), errors='coerce')
        if pd.api.types.is_numeric_dtype(converted_col) and converted_col.notnull().sum() > 0:
             df[col] = converted_col
             if df[col].dtype != original_type: converted_count += 1
    except Exception as e:
        print(f"  Warning: Could not convert {col}: {e}")
        pass
print(f"Attempted numeric conversion. {converted_count} columns changed type.")


# --- [MODIFIED] Step 3: Feature Engineering (Generic Differences) ---
print("\n--- 3. Feature Engineering (Generic Differences) ---")
target_cols = ['Target 1', 'Target 2', 'Target 3']
engineered_features = []

# Identify original numeric features BEFORE creating differences
original_numeric_cols = df.select_dtypes(include=np.number).columns.drop(
    target_cols + ['Year'], errors='ignore' # Exclude targets, Year
).tolist()
# Ensure 'Company_ID_Encoded' is not included if it exists from a previous run
if 'Company_ID_Encoded' in original_numeric_cols:
    original_numeric_cols.remove('Company_ID_Encoded')


print(f"Calculating YoY differences for {len(original_numeric_cols)} original numeric features...")
df = df.sort_values(by=['Company', 'Year']) # CRITICAL: Sort before using shift/diff

for col in original_numeric_cols:
    # Ensure the original column is numeric first
    df[col] = pd.to_numeric(df[col], errors='coerce')
    # Calculate absolute difference
    diff_col_name = f'FE_{col}_Diff'
    df[diff_col_name] = df.groupby('Company')[col].diff()
    # Convert to float32 and add to list for processing
    df[diff_col_name] = df[diff_col_name].astype(np.float32)
    engineered_features.append(diff_col_name)

print(f" - Created {len(engineered_features)} absolute difference (YoY) features.")


# --- [MODIFIED] Step 4: Separate Columns & Identify ALL Features for Processing ---
print("\n--- 4. Separating Columns & Identifying Processing Features ---")
company_col_preserved = df['Company'].copy()
year_col_preserved = df['Year'].copy()
target_df_preserved = df[target_cols].copy() # Preserve original targets

# Start with the original numeric cols + newly engineered diff cols
numeric_processing_cols = original_numeric_cols + engineered_features

# Remove any duplicates just in case, and ensure they exist
numeric_processing_cols = sorted(list(set(
    col for col in numeric_processing_cols if col in df.columns
)))

# --- Final Sanity Checks ---
if 'Year' in numeric_processing_cols: raise ValueError("FATAL: 'Year' included for processing!")
if 'Company' in numeric_processing_cols: raise ValueError("FATAL: 'Company' included for processing!")
if 'Company_ID_Encoded' in numeric_processing_cols: raise ValueError("FATAL: 'Company_ID_Encoded' included for processing!")
if any(tc in numeric_processing_cols for tc in target_cols): raise ValueError("FATAL: Targets included for processing!")
# --- End Sanity Checks ---

numeric_df_to_process = df[numeric_processing_cols].copy()
print(f"{len(numeric_processing_cols)} numeric columns identified for processing (Original + Engineered Diffs).")

# 5. KNN Imputation (Features + Engineered Features)
print("\n--- 5. Performing KNN Imputation ---")
if not numeric_df_to_process.empty and numeric_df_to_process.isnull().any().any():
    null_counts_before = numeric_df_to_process.isnull().sum()
    print(f"Nulls before imputation in {null_counts_before[null_counts_before > 0].count()} columns (Total: {null_counts_before.sum()})")
    imputer = KNNImputer(n_neighbors=N_NEIGHBORS_IMPUTE)
    numeric_df_to_process = numeric_df_to_process.astype(np.float32) # Ensure float32 before impute
    imputed_array = imputer.fit_transform(numeric_df_to_process)
    numeric_df_processed = pd.DataFrame(imputed_array, columns=numeric_processing_cols, index=numeric_df_to_process.index)
    if numeric_df_processed.isnull().sum().sum() > 0: print("WARNING: Nulls remain after KNN imputation!")
    else: print("Imputation complete.")
else:
    print("No missing values found in processing columns or no columns to process.")
    numeric_df_processed = numeric_df_to_process

# 6. Robust Scaling (Features + Engineered Features)
print("\n--- 6. Applying Robust Scaler ---")
cols_to_scale = numeric_processing_cols
if cols_to_scale and not numeric_df_processed.empty:
    feature_scaler = RobustScaler()
    scaled_data = feature_scaler.fit_transform(numeric_df_processed[cols_to_scale])
    numeric_df_scaled = pd.DataFrame(scaled_data, columns=cols_to_scale, index=numeric_df_processed.index)
    print(f"Scaled {len(cols_to_scale)} columns (Original + Diffs) using RobustScaler.")
else:
    print("No columns identified or available for scaling.")
    numeric_df_scaled = numeric_df_processed

# 7. Capping Outliers (on Scaled Features + Engineered Features)
print("\n--- 7. Capping Outliers ---")
if cols_to_scale and not numeric_df_scaled.empty:
    numeric_df_capped = numeric_df_scaled.copy()
    capped_cols_count = 0
    for col in cols_to_scale:
        col_data = numeric_df_capped[col].dropna();
        if not col_data.empty:
            col_mean=col_data.mean(); col_std=col_data.std()
            # Check for near-zero std deviation
            if pd.notna(col_std) and col_std > 1e-6:
                lower=col_mean-Z_SCORE_CAP*col_std; upper=col_mean+Z_SCORE_CAP*col_std
                min_orig=numeric_df_capped[col].min(); max_orig=numeric_df_capped[col].max()
                numeric_df_capped[col]=numeric_df_capped[col].clip(lower, upper)
                if numeric_df_capped[col].min()>min_orig or numeric_df_capped[col].max()<max_orig: capped_cols_count+=1
    if capped_cols_count > 0: print(f"Capped outliers in {capped_cols_count} columns.")
    else: print("No outliers needed capping.")
    numeric_df_final_processed = numeric_df_capped
else:
    print("No columns available for capping.")
    numeric_df_final_processed = numeric_df_scaled

# 8. Apply PCA (on combined imputed, scaled, capped features)
print(f"\n--- 8. Applying PCA (Retaining {PCA_VARIANCE_RATIO*100:.0f}% Variance) ---")
n_original_features = numeric_df_final_processed.shape[1]
pca_df = pd.DataFrame(index=df.index) # Initialize pca_df
numeric_features_for_lstm = []
n_components = 0

if n_original_features > 0:
    try:
        pca = PCA(n_components=PCA_VARIANCE_RATIO, svd_solver='full') # Use 'full' for reliability if components << features
        pca_components = pca.fit_transform(numeric_df_final_processed)
        n_components = pca.n_components_
        # Handle edge case where PCA selects 0 components (unlikely with ratio)
        if n_components > 0:
            print(f"PCA applied. Reduced features from {n_original_features} to {n_components}.")
            pc_cols = [f'PC_{i+1}' for i in range(n_components)]
            pca_df = pd.DataFrame(pca_components, columns=pc_cols, index=numeric_df_final_processed.index)
            numeric_features_for_lstm = pc_cols # LSTM will use these PC columns
        else:
             print("PCA resulted in 0 components. Skipping PCA.")
    except Exception as e:
        print(f"PCA failed: {e}. Skipping PCA.")
else:
    print("Skipping PCA: No numeric features to process.")


# 9. Reconstruct DataFrame (Using PCA Components)
print("\n--- 9. Reconstructing DataFrame with PCA Components ---")
final_df = pd.concat([
    company_col_preserved.reset_index(drop=True),
    year_col_preserved.reset_index(drop=True),
    pca_df.reset_index(drop=True),             # PCA components (might be empty)
    target_df_preserved.reset_index(drop=True) # Original Targets
], axis=1)

# 10. Scale Target Variables
print("\n--- 10. Scaling Target Variables (Separately) ---")
target_scalers = {}
final_df_scaled_targets = final_df.copy()
for target_col in target_cols:
    if target_col not in final_df_scaled_targets.columns:
        print(f"Warning: Target column '{target_col}' not found for scaling.")
        continue
    scaler = StandardScaler()
    target_data = final_df_scaled_targets[[target_col]].astype(np.float32)
    valid_target_data = target_data.dropna()
    if not valid_target_data.empty:
        try:
            scaler.fit(valid_target_data)
            # Handle cases where all values are the same after potential earlier processing
            if scaler.scale_ is not None and np.all(scaler.scale_ > 1e-8):
                 final_df_scaled_targets[target_col] = scaler.transform(target_data)
                 target_scalers[target_col] = scaler
                 print(f"Applied StandardScaler to target column: '{target_col}'")
            else:
                 print(f"Skipping scaling for '{target_col}' due to zero variance.")
                 target_scalers[target_col] = None # Indicate scaling wasn't applied
        except Exception as e:
            print(f"Error scaling target '{target_col}': {e}")
            target_scalers[target_col] = None
    else:
        print(f"No valid data to scale for target column: '{target_col}'")
        target_scalers[target_col] = None

# 11. Label Encode Company
print("\n--- 11. Encoding Company Names ---")
le = LabelEncoder()
final_df_scaled_targets['Company_ID_Encoded'] = le.fit_transform(final_df_scaled_targets['Company'])
num_companies = final_df_scaled_targets['Company_ID_Encoded'].nunique()
print(f"Encoded 'Company' into 'Company_ID_Encoded'. Found {num_companies} unique companies.")

print("\n--- Final Preprocessed Data Sample (PCA Features, Scaled Targets) ---")
print(final_df_scaled_targets.head())
print(f"\nFinal DataFrame shape: {final_df_scaled_targets.shape}")
# Final check for NaNs in PCA components
if not numeric_features_for_lstm: print("No PCA features to check.")
elif final_df_scaled_targets[numeric_features_for_lstm].isnull().any().any():
      print(f"\nWARNING: NaNs detected in final PCA columns:\n{final_df_scaled_targets[numeric_features_for_lstm].isnull().sum().sort_values(ascending=False).head()}")
else:
      print("\nNo NaNs detected in final PCA columns.")

# --- LSTM Data Preparation ---

# 12. Sort Data
print("\n--- 12. Sorting Data ---")
final_df_scaled_targets = final_df_scaled_targets.sort_values(by=['Company_ID_Encoded', 'Year']).reset_index(drop=True)

# 13. Define LSTM Features (Already defined as numeric_features_for_lstm = pc_cols)
print("\n--- 13. Defining LSTM Features (Using PCA Components) ---")
if numeric_features_for_lstm:
    print(f"Features for LSTM sequence input ({len(numeric_features_for_lstm)}): {numeric_features_for_lstm[:5]}...")
else:
    print("No PCA features generated. LSTM input will be empty if this proceeds.")

# 14. Sequence Creation Function
print("\n--- 14. Defining Sequence Creation Function ---")
def create_sequences(df, numeric_cols, target_cols, company_col_id_encoded, year_col, min_hist_years=1):
    all_sequences, all_company_ids, all_targets = [], [], []
    max_len = 0
    skipped_nan_sequences, skipped_nan_targets = 0, 0
    skipped_short_hist = 0

    # Ensure numeric_cols exist in df before proceeding
    valid_numeric_cols = [col for col in numeric_cols if col in df.columns]
    if not valid_numeric_cols:
        print("Warning: No valid numeric columns found for sequence creation.")
        return np.array([]), np.array([]), np.array([]), 0

    company_groups = df.groupby(company_col_id_encoded)

    for company_id, group in company_groups:
        group = group.sort_values(year_col)
        features = group[valid_numeric_cols].values # Use only valid columns
        targets = group[target_cols].values

        # Start from the index corresponding to the minimum history requirement
        for i in range(min_hist_years, len(group)):
            # Sequence includes data up to year i-1 (inclusive)
            current_sequence = features[:i, :]
            # Target is data for year i
            current_target = targets[i, :]

            # Check for NaNs/Infs in the current sequence slice AND target
            # Important: Check AFTER slicing, before appending
            if np.isnan(current_sequence).any() or np.isinf(current_sequence).any():
                skipped_nan_sequences += 1
                continue
            # Check target for NaNs (especially important after scaling)
            if np.isnan(current_target).any() or np.isinf(current_target).any():
                skipped_nan_targets += 1
                continue

            all_sequences.append(current_sequence)
            all_company_ids.append(company_id)
            all_targets.append(current_target)

            if current_sequence.shape[0] > max_len:
                max_len = current_sequence.shape[0]

    if skipped_short_hist > 0: print(f"Note: Skipped {skipped_short_hist} potential sequences due to insufficient history (< {min_hist_years} years).")
    if skipped_nan_sequences > 0: print(f"Warning: Skipped {skipped_nan_sequences} sequences due to NaN/Inf in features.")
    if skipped_nan_targets > 0: print(f"Warning: Skipped {skipped_nan_targets} sequences due to NaN/Inf in targets.")

    if not all_sequences:
        print("Warning: No valid sequences generated.")
        return np.array([]), np.array([]), np.array([]), 0

    # Pad sequences
    # Use 0.0 as padding value, handled by Masking layer
    X_padded = pad_sequences(all_sequences, maxlen=max_len, dtype='float32', padding='pre', truncating='pre', value=0.0)
    y_array = np.array(all_targets, dtype='float32')
    X_comp_array = np.array(all_company_ids, dtype='int32')

    # Final check for NaNs introduced unexpectedly (e.g., during padding if value wasn't float)
    if np.isnan(X_padded).any(): print("FATAL WARNING: NaNs found in X_padded after padding!")
    if np.isnan(y_array).any(): print("FATAL WARNING: NaNs found in y_array after collection!")

    return X_padded, X_comp_array, y_array, max_len


# --- [NEW] Encoder-Decoder Model Architecture Function ---
print("\n--- 15. Defining Encoder-Decoder LSTM Model Architecture Function ---")

def build_encoder_decoder_model(max_sequence_length, num_numeric_features, num_companies, num_targets_out,
                                embedding_dim=EMBEDDING_DIM, encoder_lstm_units=ENCODER_LSTM_UNITS,
                                dense_units=DENSE_UNITS, dropout_rate=DROPOUT_RATE, learning_rate=LEARNING_RATE):
    """
    Builds an Encoder-Decoder LSTM model for single-step prediction.

    Args:
        max_sequence_length: Maximum length of input sequences.
        num_numeric_features: Number of features in the input sequence (e.g., PCA components).
        num_companies: Total number of unique companies for embedding.
        num_targets_out: Number of target variables to predict (usually 1 for separate models).
        embedding_dim: Dimension of the company embedding.
        encoder_lstm_units: Number of units in the Encoder LSTM layer.
        dense_units: Number of units in the Dense layer after concatenation.
        dropout_rate: Dropout rate.
        learning_rate: Learning rate for the optimizer.

    Returns:
        A compiled Keras Model.
    """
    # --- Inputs ---
    sequence_input = Input(shape=(max_sequence_length, num_numeric_features), name='Sequence_Input')
    company_input = Input(shape=(1,), name='Company_Input')

    # --- Company Embedding ---
    company_emb = Embedding(input_dim=num_companies, output_dim=embedding_dim,
                            embeddings_initializer='he_normal', name='Company_Embedding')(company_input)
    company_emb_flat = Flatten(name='Flatten_Embedding')(company_emb) # Flatten for concatenation

    # --- Encoder ---
    # Masking layer to ignore padded parts of the sequence (value=0.0)
    masked_input = Masking(mask_value=0.0, name='Masking')(sequence_input)
    # Encoder LSTM processes the sequence and returns its final states
    # We need return_state=True to get the context vector (hidden state h and cell state c)
    encoder_lstm = LSTM(encoder_lstm_units, kernel_initializer='he_normal',
                        return_sequences=False, # We only need the final state, not the sequence output
                        return_state=True,      # Crucial for getting context
                        name='Encoder_LSTM')
    # The outputs are: last_output (same as state_h if return_sequences=False), state_h, state_c
    _, encoder_state_h, encoder_state_c = encoder_lstm(masked_input)

    # The context vector can be just state_h, or state_c, or both concatenated.
    # Using state_h is common.
    encoder_context = encoder_state_h

    # --- "Decoder" (Simplified for single-step prediction) ---
    # Combine the encoder's context vector with the company embedding
    merged = Concatenate(name='Concatenate_Context_Embedding')([encoder_context, company_emb_flat])

    # Dense layers to process the combined information
    x = Dense(dense_units, activation='relu', kernel_initializer='he_normal', name='Dense_1')(merged)
    x = Dropout(dropout_rate, name='Dropout_1')(x)
    # Optional: Add another Dense layer if needed
    # x = Dense(dense_units // 2, activation='relu', kernel_initializer='he_normal', name='Dense_2')(x)
    # x = Dropout(dropout_rate, name='Dropout_2')(x)

    # --- Output Layer ---
    # Final Dense layer to predict the target(s)
    output = Dense(num_targets_out, name='Output_Layer')(x) # Linear activation for regression

    # --- Build and Compile Model ---
    model = Model(inputs=[sequence_input, company_input], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='mse',
                  metrics=[RootMeanSquaredError(name='rmse')])

    return model


# --- Custom Training Loop (Input features are PCs, Model is Encoder-Decoder) ---
print("\n--- 16. Starting Expanding Window Training (PCA Features, Encoder-Decoder Model) ---")
results = {}
unique_years = sorted(final_df_scaled_targets['Year'].unique())
print(f"Unique years found: {unique_years}")

if not numeric_features_for_lstm:
     print("\nSTOPPING: No PCA features were generated. Cannot train the LSTM model.")
elif len(unique_years) <= MIN_HISTORY_YEARS:
    print(f"Not enough distinct years ({len(unique_years)}) for training with min history {MIN_HISTORY_YEARS}.")
else:
    # Outer loop: Iterate through years
    for test_year_index in range(MIN_HISTORY_YEARS, len(unique_years)):
        test_year = unique_years[test_year_index]
        train_end_year = unique_years[test_year_index - 1]
        print(f"\n{'='*10} Processing Fold for Test Year: {int(test_year)} (Train up to {int(train_end_year)}) {'='*10}")

        # 1. Filter & Create base sequences (using PCA columns)
        train_df = final_df_scaled_targets[final_df_scaled_targets['Year'] <= train_end_year].copy()
        X_seq_train, X_cid_train, y_train_scaled_all, max_seq_len_train = create_sequences(
            train_df, numeric_features_for_lstm, target_cols, 'Company_ID_Encoded', 'Year', min_hist_years=MIN_HISTORY_YEARS)

        if X_seq_train.shape[0] == 0 or max_seq_len_train == 0:
            print(f"No valid training sequences generated for fold ending {int(train_end_year)}. Skipping fold.")
            continue
        print(f"Generated {X_seq_train.shape[0]} base training sequences (Input: {X_seq_train.shape[-1]} PCs, Max Len: {max_seq_len_train}).")

        # 2. Prepare base test sequences (using PCA columns)
        test_sequences, test_company_ids, test_actuals_scaled_all = [], [], []
        test_candidate_df = final_df_scaled_targets[final_df_scaled_targets['Year'] <= test_year].copy()

        skipped_test_nan_seq, skipped_test_nan_target, skipped_test_short = 0, 0, 0

        for company_id, group in test_candidate_df.groupby('Company_ID_Encoded'):
            # Check if the company exists in the test year
            if test_year in group['Year'].values:
                 # History is data strictly BEFORE the test year
                 group_hist = group[group['Year'] < test_year] # Use < test_year

                 # Ensure enough history EXCLUDING the test year
                 if not group_hist.empty and len(group_hist) >= MIN_HISTORY_YEARS:
                     # Sequence is the history up to train_end_year
                     current_sequence = group_hist[numeric_features_for_lstm].values # Use PCA feature names

                     # Target(s) are from the specific test year
                     actual_targets_scaled_row = group[group['Year'] == test_year][target_cols].values
                     if actual_targets_scaled_row.shape[0] == 0: continue # Should not happen if test_year check passed, but safety
                     actual_targets_scaled = actual_targets_scaled_row.flatten()

                     # --- Check for NaNs/Infs AFTER creating sequence/target ---
                     if np.isnan(current_sequence).any() or np.isinf(current_sequence).any():
                         skipped_test_nan_seq += 1
                         continue
                     if np.isnan(actual_targets_scaled).any() or np.isinf(actual_targets_scaled).any():
                         # This check is crucial, esp. if target scaling failed for some targets
                         skipped_test_nan_target += 1
                         continue

                     test_sequences.append(current_sequence)
                     test_company_ids.append(company_id)
                     test_actuals_scaled_all.append(actual_targets_scaled)
                 else:
                    skipped_test_short += 1

        if skipped_test_nan_seq > 0: print(f"Warning: Skipped {skipped_test_nan_seq} test sequences due to NaN/Inf in features.")
        if skipped_test_nan_target > 0: print(f"Warning: Skipped {skipped_test_nan_target} test sequences due to NaN/Inf in targets.")
        if skipped_test_short > 0: print(f"Note: Skipped {skipped_test_short} potential test companies due to insufficient history.")

        if not test_sequences:
            print(f"No valid test sequences generated for test year {int(test_year)}. Skipping prediction for this fold.")
            continue

        # Pad test sequences using the MAX length determined from TRAINING data
        X_seq_test = pad_sequences(test_sequences, maxlen=max_seq_len_train, dtype='float32', padding='pre', truncating='pre', value=0.0)
        X_cid_test = np.array(test_company_ids, dtype='int32')
        y_test_actual_scaled_all = np.array(test_actuals_scaled_all, dtype='float32')

        # Final checks on padded test data
        if np.isnan(X_seq_test).any(): print(f"FATAL WARNING: NaNs found in X_seq_test for year {int(test_year)}!")
        if np.isnan(y_test_actual_scaled_all).any(): print(f"FATAL WARNING: NaNs found in y_test_actual_scaled_all for year {int(test_year)}!")

        print(f"Prepared {X_seq_test.shape[0]} base valid test sequences (Input: {X_seq_test.shape[-1]} PCs).")

        # --- Inner loop: Train separate model for each target ---
        results[test_year] = {}
        for target_idx, target_name in enumerate(target_cols):
            print(f"\n--- Training for Target: {target_name} (Year: {int(test_year)}) ---")

            # Select the specific target column for this model
            y_train_target_scaled = y_train_scaled_all[:, target_idx]
            y_test_actual_target_scaled = y_test_actual_scaled_all[:, target_idx]

            # Check if target scaler exists and is valid
            if target_name not in target_scalers or target_scalers[target_name] is None:
                print(f"Scaler for {target_name} not found or invalid (e.g., due to zero variance). Skipping target.")
                results[test_year][target_name] = {'predictions_original': np.full(X_seq_test.shape[0], np.nan),
                                                   'actuals_original': np.full(X_seq_test.shape[0], np.nan)} # Store NaNs
                continue

            # Check if there are non-NaN values in the target data for training
            if np.isnan(y_train_target_scaled).all():
                 print(f"All training target values for {target_name} are NaN. Skipping target.")
                 results[test_year][target_name] = {'predictions_original': np.full(X_seq_test.shape[0], np.nan),
                                                   'actuals_original': np.full(X_seq_test.shape[0], np.nan)} # Store NaNs
                 continue


            # Build ENCODER-DECODER model (passing n_components)
            current_n_features = n_components # From PCA step
            if current_n_features == 0:
                print("No PCA features to train on. Skipping model building.")
                # Should have been caught earlier, but safe check
                continue

            # *** Use the new model builder ***
            model = build_encoder_decoder_model(
                max_sequence_length=max_seq_len_train,
                num_numeric_features=current_n_features,
                num_companies=num_companies,
                num_targets_out=1, # Predict one target at a time
                embedding_dim=EMBEDDING_DIM,
                encoder_lstm_units=ENCODER_LSTM_UNITS,
                dense_units=DENSE_UNITS,
                dropout_rate=DROPOUT_RATE,
                learning_rate=LEARNING_RATE
            )
            # Optional: Print model summary for the first fold/target
            # if test_year_index == MIN_HISTORY_YEARS and target_idx == 0:
            #    model.summary()

            # Callbacks
            early_stopping = EarlyStopping(monitor='val_rmse', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=0, mode='min')
            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.2, patience=REDUCE_LR_PATIENCE, min_lr=1e-6, verbose=0, mode='min')

            # Train model
            print(f"Training model for {target_name}...")
            # Ensure no NaNs in training inputs/outputs before fitting
            train_mask = ~np.isnan(y_train_target_scaled)
            if not np.any(train_mask):
                 print(f"No valid (non-NaN) training targets for {target_name}. Cannot train model.")
                 results[test_year][target_name] = {'predictions_original': np.full(X_seq_test.shape[0], np.nan),
                                                   'actuals_original': np.full(X_seq_test.shape[0], np.nan)} # Store NaNs
                 del model; tf.keras.backend.clear_session(); gc.collect()
                 continue

            X_seq_train_filt = X_seq_train[train_mask]
            X_cid_train_filt = X_cid_train[train_mask]
            y_train_target_scaled_filt = y_train_target_scaled[train_mask]

            history = model.fit([X_seq_train_filt, X_cid_train_filt.reshape(-1, 1)], y_train_target_scaled_filt,
                                epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,
                                callbacks=[early_stopping, reduce_lr], verbose=0) # Use verbose=0 for cleaner logs

            epochs_trained = len(history.history['loss'])
            best_val_rmse = min(history.history.get('val_rmse', [np.inf])) # Handle case where validation split might be 0
            print(f"Training complete ({epochs_trained} epochs). Best Val RMSE (Scaled): {best_val_rmse:.4f}")

            # Predict (scaled)
            # Use the original test set indices, as predictions are needed for all valid test samples
            predictions_scaled = model.predict([X_seq_test, X_cid_test.reshape(-1, 1)], verbose=0)

            # Inverse Transform
            scaler_t = target_scalers[target_name]
            predictions_original = np.full(predictions_scaled.shape[0], np.nan) # Initialize with NaNs
            y_test_actual_original = np.full(y_test_actual_target_scaled.shape[0], np.nan)

            # Inverse transform only where the actual value was not NaN
            test_actual_valid_mask = ~np.isnan(y_test_actual_target_scaled)
            if np.any(test_actual_valid_mask):
                 try:
                     predictions_original[test_actual_valid_mask] = scaler_t.inverse_transform(predictions_scaled[test_actual_valid_mask]).flatten()
                     y_test_actual_original[test_actual_valid_mask] = scaler_t.inverse_transform(y_test_actual_target_scaled[test_actual_valid_mask].reshape(-1, 1)).flatten()
                 except ValueError as ve:
                      print(f"WARNING: ValueError during inverse transform for {target_name} (likely shape mismatch or NaN issue post-prediction): {ve}")
                      # Keep results as NaNs
                 except Exception as e:
                      print(f"ERROR during inverse transform for {target_name}: {e}. Storing NaNs.")
                      # Keep results as NaNs
            else:
                print(f"No valid actual test values for {target_name} in this fold to inverse transform.")


            # Store results
            results[test_year][target_name] = {'predictions_original': predictions_original,
                                               'actuals_original': y_test_actual_original}

            # Evaluate fold performance (Original Scale) - Only on non-NaN pairs
            valid_comparison_mask = ~np.isnan(predictions_original) & ~np.isnan(y_test_actual_original)
            if np.any(valid_comparison_mask):
                fold_rmse_orig = np.sqrt(np.mean((predictions_original[valid_comparison_mask] - y_test_actual_original[valid_comparison_mask])**2))
                print(f"  --> Test RMSE for {target_name} (Original Scale, {np.sum(valid_comparison_mask)} valid pairs): {fold_rmse_orig:.4f}")
            else:
                print(f"  --> No valid (non-NaN) prediction/actual pairs to calculate RMSE for {target_name} in this fold.")


            # Clean up memory
            del model, history, X_seq_train_filt, X_cid_train_filt, y_train_target_scaled_filt
            tf.keras.backend.clear_session()
            gc.collect()

# --- Post-Loop Analysis (Separated per target) ---
print(f"\n{'='*20} Training Loop Finished {'='*20}")
if results:
    print("\nCalculating Overall Performance Per Target (Original Scale):")
    overall_metrics = {}

    for target_idx, target_name in enumerate(target_cols):
        print(f"\n--- Overall Performance for Target: {target_name} ---")
        all_actuals_orig_list = []
        all_predictions_orig_list = []
        nan_folds_report = []

        for year in sorted(results.keys()):
            if year in results and target_name in results[year]:
                data = results[year][target_name]
                pred_orig = data['predictions_original']
                act_orig = data['actuals_original']

                # Filter out NaN pairs before appending
                valid_mask = ~np.isnan(pred_orig) & ~np.isnan(act_orig)
                if np.any(valid_mask):
                    all_actuals_orig_list.append(act_orig[valid_mask])
                    all_predictions_orig_list.append(pred_orig[valid_mask])
                # Report if the fold had *any* results, even if all were NaN after filtering
                if pred_orig.size > 0 and not np.any(valid_mask):
                     nan_folds_report.append(int(year)) # Fold generated results, but all were NaN pairs
                elif pred_orig.size == 0: # Fold didn't generate results for this target
                     nan_folds_report.append(int(year))


        if all_actuals_orig_list:
            all_actuals_np = np.concatenate(all_actuals_orig_list, axis=0)
            all_predictions_np = np.concatenate(all_predictions_orig_list, axis=0)

            if all_actuals_np.size > 0: # Ensure there's data after concatenation
                final_mse = np.mean((all_predictions_np - all_actuals_np)**2)
                final_rmse = np.sqrt(final_mse)
                final_mae = np.mean(np.abs(all_predictions_np - all_actuals_np))

                overall_metrics[target_name] = {'RMSE': final_rmse, 'MAE': final_mae}

                print(f"  Final Overall RMSE (Original Scale): {final_rmse:.4f}")
                print(f"  Final Overall MAE (Original Scale): {final_mae:.4f}")
                print(f"  Total valid prediction pairs across all folds: {all_actuals_np.size}")
                if nan_folds_report:
                    print(f"  Note: Folds for test years {nan_folds_report} produced no valid (non-NaN) comparison pairs for this target.")
            else:
                 print(f"  No valid (non-NaN) prediction/actual pairs found across all folds for {target_name}.")
                 overall_metrics[target_name] = {'RMSE': np.nan, 'MAE': np.nan}

        else:
            print(f"  No valid predictions generated across any fold for {target_name}.")
            overall_metrics[target_name] = {'RMSE': np.nan, 'MAE': np.nan}
            if nan_folds_report:
                 print(f"  Note: Folds for test years {nan_folds_report} were processed but yielded no valid data for this target.")


    print("\n--- Summary of Overall Metrics (Original Scale) ---")
    metrics_df = pd.DataFrame(overall_metrics).T
    print(metrics_df)

else:
    print("No results were generated during the training loop.")

print("\n--- Script Complete ---")



"""LSTM -2"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA # Keep PCA
from sklearn.metrics import mean_squared_error # For final evaluation
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding,
                                     Concatenate, Masking, Flatten, Dropout)
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
import gc
import matplotlib.pyplot as plt # Added for potential future plots
import seaborn as sns        # Added for potential future plots

# --- Configuration ---
# Preprocessing
N_NEIGHBORS_IMPUTE = 5
PCA_VARIANCE_RATIO = 0.95 # Keep PCA variance threshold
# Model Hyperparameters
EMBEDDING_DIM = 16
LSTM_UNITS_L1 = 64
LSTM_UNITS_L2 = 32
DENSE_UNITS = 32
DROPOUT_RATE = 0.2
LEARNING_RATE = 5e-4
# Training Parameters
EPOCHS = 50 # Reduced epochs, relying more on early stopping
BATCH_SIZE = 64
VALIDATION_SPLIT = 0.15
EARLY_STOPPING_PATIENCE = 10
REDUCE_LR_PATIENCE = 5
MIN_HISTORY_YEARS = 3

# --- !! Ensure full run !! ---
# RUN_ONLY_FIRST_FOLD = False # Make sure this is commented out or False

warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
tf.get_logger().setLevel('ERROR') # Quieter TF logs
tf.autograph.set_verbosity(0)

# --- Load Data ---
# Assume df is loaded (using the sample data generation if not)
try:
    df.head() # Check if df exists
    print("--- Using pre-loaded DataFrame ---")
except NameError:
    print("--- Creating Sample Data ---")
    num_companies = 50
    years = range(2010, 2024)
    data = []
    for i in range(num_companies):
        company_name = f"Company_{i:03d}"
        base_feature1 = np.random.rand() * 100
        base_feature2 = np.random.rand() * 50
        trend1 = (np.random.rand() - 0.5) * 5
        trend2 = (np.random.rand() - 0.5) * 3
        target_base1 = np.random.rand() * 10
        target_base2 = np.random.rand() * 5
        target_base3 = np.random.rand() * 20
        for j, year in enumerate(years):
            row = {'Company': company_name, 'Year': year}
            row['Feature1'] = base_feature1 + trend1 * j + np.random.normal(0, 10)
            row['Feature2'] = base_feature2 + trend2 * j + np.random.normal(0, 5)
            row['Feature3'] = np.random.rand() * 200 # Example base feature
            # Add more base features if needed (e.g., Feature4 to Feature28)
            for k in range(4, 29): # Simulate Features 4 to 28
                 row[f'Feature{k}'] = np.random.rand() * (k*5) + np.random.normal(0, k)

            noise = np.random.normal(0, 2, size=3)
            row['Target1'] = target_base1 + 0.1*row['Feature1'] + 0.05*row['Feature2'] + 0.5*j + noise[0]
            row['Target2'] = target_base2 - 0.05*row['Feature1'] + 0.1*row['Feature2'] + 0.2*j + noise[1]
            row['Target3'] = target_base3 + 0.02*row.get('Feature3',0) - 0.1*j + noise[2]

            # Introduce missing values randomly
            if np.random.rand() < 0.1: row['Feature1'] = np.nan
            if np.random.rand() < 0.05: row['Feature2'] = np.nan
            if np.random.rand() < 0.15: row['Feature3'] = np.nan
            # Simulate NaNs in other features
            for k in range(4, 29):
                 if np.random.rand() < 0.08: row[f'Feature{k}'] = np.nan

            if np.random.rand() < 0.05 and j > 0 : row['Target1'] = np.nan
            if np.random.rand() < 0.08 and j > 0 : row['Target2'] = np.nan
            if np.random.rand() < 0.10 and j > 0 : row['Target3'] = np.nan

            data.append(row)
    df = pd.DataFrame(data)
    # # # Remove this if you want to test the script's FE creation properly
    # # df['FE_Feature1_Diff'] = df.groupby('Company')['Feature1'].diff() # Example of pre-existing FE

print("--- Initial Data Sample (from df) ---")
print(df.head())
print(f"\nInitial DataFrame shape: {df.shape}")
print(f"Initial NaNs:\n{df.isnull().sum()[df.isnull().sum() > 0]}")


# --- Preprocessing ---

# 1. Clean column names
print("\n--- 1. Cleaning Column Names ---")
df.columns = df.columns.str.strip().str.replace('[^A-Za-z0-9_]+', '', regex=True)
print(f"Cleaned columns: {df.columns.tolist()}")

# 2. Convert object columns to numeric where possible
print("\n--- 2. Converting Columns to Numeric ---")
potential_numeric_cols = df.select_dtypes(include='object').columns
if 'Company' in potential_numeric_cols:
     potential_numeric_cols = potential_numeric_cols.drop('Company')

converted_count = 0
cols_converted = []
for col in potential_numeric_cols:
    if col not in df.columns: continue
    original_type = df[col].dtype
    converted_col = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce')
    if pd.api.types.is_numeric_dtype(converted_col) and converted_col.notnull().sum() > 0.5 * len(converted_col):
         df[col] = converted_col
         if df[col].dtype != original_type:
             converted_count += 1
             cols_converted.append(col)
print(f"Attempted numeric conversion. {converted_count} columns potentially changed type: {cols_converted}")
print(f"NaNs after conversion attempt:\n{df.isnull().sum()[df.isnull().sum() > 0]}")

# --- 3. Define Target Columns & Identify Base Features ---
target_cols = ['Target1', 'Target2', 'Target3']
target_cols = [col for col in target_cols if col in df.columns]
if not target_cols:
    raise ValueError("FATAL: No target columns found in the DataFrame after cleaning.")
print(f"\n--- Identified Target Columns: {target_cols} ---")

# Identify BASE numeric features BEFORE creating differences
# Exclude any pre-existing 'FE_' columns from this initial list
base_numeric_cols = df.select_dtypes(include=np.number).columns
base_numeric_cols = [
    col for col in base_numeric_cols
    if col not in target_cols + ['Year'] and not col.startswith('FE_')
]
print(f"Identified {len(base_numeric_cols)} base numeric features for potential processing: {base_numeric_cols[:5]}...")

# --- 4. Feature Engineering (Difference Features - MODIFIED) ---
print("\n--- 4. Feature Engineering (Generic Differences) ---")
engineered_features = [] # Store names of FE features (existing or created)

print(f"Calculating YoY differences for {len(base_numeric_cols)} base numeric features...")
df = df.sort_values(by=['Company', 'Year']) # CRITICAL: Sort

# Ensure numeric types before diff
for col in base_numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Calculate Differences ONLY for base features
newly_created_fe_count = 0
for col in base_numeric_cols: # Iterate only over BASE features
    if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
        diff_col_name = f'FE_{col}_Diff'

        # Only create if it doesn't already exist from input
        if diff_col_name not in df.columns:
            df[diff_col_name] = df.groupby('Company')[col].diff()
            df[diff_col_name] = df[diff_col_name].astype(np.float32)
            engineered_features.append(diff_col_name)
            newly_created_fe_count += 1
        elif diff_col_name not in engineered_features:
             # If it already exists, still track it for processing later
             engineered_features.append(diff_col_name)
             # Ensure its type is float32
             df[diff_col_name] = pd.to_numeric(df[diff_col_name], errors='coerce').astype(np.float32)


print(f" - Created {newly_created_fe_count} new difference (YoY) features.")
print(f" - Identified/tracked {len(engineered_features)} total difference features.")
print(f"NaNs after Feature Engineering:\n{df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False).head()}")


# --- 5. Separate Columns & Identify ALL Features for Processing (MODIFIED) ---
print("\n--- 5. Separating Columns & Identifying Processing Features ---")
company_col_preserved = df['Company'].copy()
year_col_preserved = df['Year'].copy()
target_df_preserved = df[target_cols].copy() # Preserve original targets

# Identify ALL numeric columns available now (base + engineered)
all_numeric_cols_after_fe = df.select_dtypes(include=np.number).columns.tolist()

# Define the columns to actually use for processing (base + FE)
# Exclude targets, Year, Company explicitly
numeric_processing_cols = [
    col for col in all_numeric_cols_after_fe
    if col not in target_cols + ['Year', 'Company']
]
# Keep unique and sort (handles case where FE features might have been in input)
numeric_processing_cols = sorted(list(set(numeric_processing_cols)))

# --- Final Sanity Checks (Redundant if definition above is correct, but safe) ---
if 'Year' in numeric_processing_cols:
    print("Warning: 'Year' detected in numeric processing columns, removing.")
    numeric_processing_cols.remove('Year')
if 'Company' in numeric_processing_cols:
    print("Warning: 'Company' detected in numeric processing columns, removing.")
    numeric_processing_cols.remove('Company')
numeric_processing_cols = [col for col in numeric_processing_cols if col not in target_cols]
# --- End Sanity Checks ---

if not numeric_processing_cols:
     raise ValueError("FATAL: No numeric columns left for processing after filtering.")

# Keep only the BASE features and the FIRST-ORDER FE features
numeric_processing_cols = [
    col for col in numeric_processing_cols if not col.startswith('FE_FE_')
]

numeric_df_to_process = df[numeric_processing_cols].astype(np.float32).copy() # Ensure float32
print(f"{len(numeric_processing_cols)} numeric columns identified for processing: {numeric_processing_cols[:5]}...")
print(f"Shape of numeric_df_to_process: {numeric_df_to_process.shape}")


# 6. KNN Imputation (Features + Engineered Features)
print("\n--- 6. Performing KNN Imputation ---")
if not numeric_df_to_process.empty and numeric_df_to_process.isnull().any().any():
    null_counts_before = numeric_df_to_process.isnull().sum()
    cols_with_nulls = null_counts_before[null_counts_before > 0]
    print(f"Nulls before imputation in {len(cols_with_nulls)} columns (Total: {null_counts_before.sum()}). Top 5:\n{cols_with_nulls.sort_values(ascending=False).head()}")

    if null_counts_before.sum() > 0:
        imputer = KNNImputer(n_neighbors=N_NEIGHBORS_IMPUTE)
        # --- Add progress print for long imputation ---
        print("Starting KNNImputer fit_transform (this may take time)...")
        imputed_array = imputer.fit_transform(numeric_df_to_process)
        print("KNNImputer fit_transform finished.")
        # --- End progress print ---
        numeric_df_processed = pd.DataFrame(imputed_array, columns=numeric_processing_cols, index=numeric_df_to_process.index)

        if numeric_df_processed.isnull().sum().sum() > 0:
            print("WARNING: Nulls remain after KNN imputation! Filling remaining with 0.")
            numeric_df_processed = numeric_df_processed.fillna(0)
        else:
            print("Imputation complete.")
    else:
         print("No missing values requiring imputation.")
         numeric_df_processed = numeric_df_to_process # Pass through if no nulls
else:
    print("No missing values found in processing columns or no columns to process.")
    numeric_df_processed = numeric_df_to_process

# 7. Robust Scaling (Features + Engineered Features)
print("\n--- 7. Applying Robust Scaler ---")
if not numeric_df_processed.empty:
    feature_scaler = RobustScaler()
    scaled_data = feature_scaler.fit_transform(numeric_df_processed)
    numeric_df_scaled = pd.DataFrame(scaled_data, columns=numeric_processing_cols, index=numeric_df_processed.index)
    print(f"Scaled {len(numeric_processing_cols)} columns using RobustScaler.")
else:
    print("No columns identified or available for scaling.")
    numeric_df_scaled = numeric_df_processed # Pass through if no scaling happened

# 8. Skipping explicit outlier capping (using RobustScaler)
numeric_df_final_processed = numeric_df_scaled
print("--- 8. Skipping explicit outlier capping (using RobustScaler) ---")


# 9. Apply PCA (on combined imputed, scaled features)
print(f"\n--- 9. Applying PCA (Retaining {PCA_VARIANCE_RATIO*100:.0f}% Variance) ---")
n_original_features = numeric_df_final_processed.shape[1]
pca_model = None
pca_df = pd.DataFrame(index=numeric_df_final_processed.index)
numeric_features_for_lstm = []
n_components = 0

if n_original_features > 1:
    try:
        pca_model = PCA(n_components=PCA_VARIANCE_RATIO, svd_solver='full')
        if numeric_df_final_processed.isnull().any().any():
            print("WARNING: NaNs detected before PCA. Filling with 0.")
            numeric_df_final_processed = numeric_df_final_processed.fillna(0)

        print("Starting PCA fit_transform...")
        pca_components = pca_model.fit_transform(numeric_df_final_processed)
        print("PCA fit_transform finished.")
        n_components = pca_model.n_components_
        print(f"PCA applied. Reduced features from {n_original_features} to {n_components}.")
        pc_cols = [f'PC_{i+1}' for i in range(n_components)]
        pca_df = pd.DataFrame(pca_components, columns=pc_cols, index=numeric_df_final_processed.index)
        numeric_features_for_lstm = pc_cols
    except Exception as e:
        print(f"ERROR during PCA: {e}. Skipping PCA.")
        raise ValueError(f"PCA failed: {e}")

elif n_original_features > 0:
     print("Skipping PCA: Only one feature available. Using the original scaled feature.")
     pca_df = numeric_df_final_processed.copy()
     numeric_features_for_lstm = numeric_processing_cols
     n_components = len(numeric_features_for_lstm)
else:
    print("Skipping PCA: No numeric features to process.")

if n_components == 0 and len(numeric_processing_cols) > 0:
     print("PCA resulted in 0 components, but features existed. Check variance or data. Stopping.")
     raise ValueError("PCA resulted in 0 components. Cannot proceed.")
elif n_components == 0 and len(numeric_processing_cols) == 0 : # check if processing_cols was also 0
     print("No features available for LSTM input after PCA/processing. Stopping.")
     raise ValueError("No features available for LSTM.")
elif n_components == 0: # Handles case where n_components=0 but processing_cols > 0
    print("PCA resulted in 0 components. Using original scaled features instead.")
    pca_df = numeric_df_final_processed.copy()
    numeric_features_for_lstm = numeric_processing_cols
    n_components = len(numeric_features_for_lstm)


# 10. Reconstruct DataFrame (Using PCA Components)
print("\n--- 10. Reconstructing DataFrame with Processed Components ---")
pca_df = pca_df.reset_index(drop=True)
company_col_preserved = company_col_preserved.reset_index(drop=True)
year_col_preserved = year_col_preserved.reset_index(drop=True)
target_df_preserved = target_df_preserved.reset_index(drop=True)

final_df = pd.concat([
    company_col_preserved,
    year_col_preserved,
    pca_df,             # PCA components (or original scaled if PCA failed/skipped/0 components)
    target_df_preserved # Original Targets
], axis=1)

del numeric_df_to_process, numeric_df_processed, numeric_df_scaled, numeric_df_final_processed
if 'pca_components' in locals(): del pca_components
gc.collect()

# 11. Scale Target Variables
print("\n--- 11. Scaling Target Variables (Separately) ---")
target_scalers = {}
final_df_scaled_targets = final_df.copy()
for target_col in target_cols:
    if target_col not in final_df_scaled_targets.columns:
        print(f"Warning: Target column '{target_col}' not found in final_df.")
        continue
    scaler = StandardScaler()
    target_data = final_df_scaled_targets[[target_col]].astype(np.float32)
    valid_target_data = target_data.dropna()
    if not valid_target_data.empty:
        scaler.fit(valid_target_data)
        final_df_scaled_targets[target_col] = scaler.transform(target_data)
        target_scalers[target_col] = scaler
        print(f"Applied StandardScaler to target column: '{target_col}'")
    else:
        print(f"Warning: No valid data found for target column '{target_col}'. Cannot scale.")
        target_scalers[target_col] = None

# 12. Label Encode Company
print("\n--- 12. Encoding Company Names ---")
le = LabelEncoder()
final_df_scaled_targets['Company_ID_Encoded'] = le.fit_transform(final_df_scaled_targets['Company'])
num_companies = final_df_scaled_targets['Company_ID_Encoded'].nunique()
company_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(f"Encoded 'Company' into 'Company_ID_Encoded'. Found {num_companies} unique companies.")

print("\n--- Final Preprocessed Data Sample (Input for Sequences) ---")
print(final_df_scaled_targets.head())
print(f"\nFinal DataFrame shape: {final_df_scaled_targets.shape}")

if numeric_features_for_lstm:
    pca_nan_check = final_df_scaled_targets[numeric_features_for_lstm].isnull().sum()
    if pca_nan_check.any():
        print(f"\nWARNING: NaNs detected in final LSTM input columns:\n{pca_nan_check[pca_nan_check > 0]}")
        raise ValueError("NaNs found in LSTM input columns after processing - check imputation/PCA step.")

target_nan_check = final_df_scaled_targets[target_cols].isnull().sum()
if target_nan_check.any():
    print(f"\nINFO: NaNs present in scaled target columns (will be handled during sequence creation):\n{target_nan_check[target_nan_check > 0]}")


# --- LSTM Data Preparation ---

# 13. Sort Data
print("\n--- 13. Sorting Data ---")
final_df_scaled_targets = final_df_scaled_targets.sort_values(by=['Company_ID_Encoded', 'Year']).reset_index(drop=True)

# 14. Define LSTM Features (using PCA components or original scaled features)
print("\n--- 14. Defining LSTM Features ---")
print(f"Features for LSTM sequence input ({len(numeric_features_for_lstm)}): {numeric_features_for_lstm[:5]}...")

# 15. Sequence Creation Function
print("\n--- 15. Defining Sequence Creation Function ---")
def create_sequences(df, numeric_cols, target_cols, company_col_id_encoded, year_col, min_hist_years=1):
    all_sequences, all_company_ids, all_targets = [], [], []
    max_len = 0
    skipped_nan_sequences, skipped_nan_targets = 0, 0
    original_indices = []

    numeric_cols = [col for col in numeric_cols if col in df.columns]
    if not numeric_cols:
         print("Warning in create_sequences: No numeric columns provided or found.")
         return np.array([]), np.array([]), np.array([]), 0, []

    company_groups = df.groupby(company_col_id_encoded)
    for company_id, group in company_groups:
        group = group.sort_values(year_col)
        features = group[numeric_cols].values
        targets = group[target_cols].values
        indices = group.index.tolist()

        for i in range(min_hist_years, len(group)):
            current_sequence = features[:i, :]
            current_target = targets[i, :]
            original_idx = indices[i]

            if np.isnan(current_sequence).any() or np.isinf(current_sequence).any():
                skipped_nan_sequences += 1
                continue
            if np.isnan(current_target).any() or np.isinf(current_target).any():
                skipped_nan_targets += 1
                continue

            all_sequences.append(current_sequence)
            all_company_ids.append(company_id)
            all_targets.append(current_target)
            original_indices.append(original_idx)

            if current_sequence.shape[0] > max_len:
                max_len = current_sequence.shape[0]

    if skipped_nan_sequences > 0:
        print(f"Warning: Skipped {skipped_nan_sequences} sequences due to NaN/Inf in feature history.")
    if skipped_nan_targets > 0:
        print(f"Warning: Skipped {skipped_nan_targets} sequences due to NaN/Inf in target values.")

    if not all_sequences:
        print("Warning: No valid sequences generated.")
        return np.array([]), np.array([]), np.array([]), 0, []

    print(f"Padding sequences to max_len={max_len}...")
    X_padded = pad_sequences(all_sequences, maxlen=max_len, dtype='float32', padding='pre', truncating='pre', value=0.0)
    print("Padding complete.")

    y_array = np.array(all_targets, dtype='float32')
    X_comp_array = np.array(all_company_ids, dtype='int32')
    original_indices_array = np.array(original_indices, dtype='int32')

    if np.isnan(X_padded).any(): print("FATAL WARNING: NaNs detected in X_padded after padding!")
    if np.isnan(y_array).any(): print("FATAL WARNING: NaNs detected in y_array after collection!")

    return X_padded, X_comp_array, y_array, max_len, original_indices_array


# 16. Define Model Architecture Function
print("\n--- 16. Defining LSTM Model Architecture Function ---")
def build_lstm_model(max_sequence_length, num_numeric_features, num_companies, num_targets_out,
                     embedding_dim=EMBEDDING_DIM, lstm_units_l1=LSTM_UNITS_L1, lstm_units_l2=LSTM_UNITS_L2,
                     dense_units=DENSE_UNITS, dropout_rate=DROPOUT_RATE, learning_rate=LEARNING_RATE):
    if num_numeric_features <= 0:
        raise ValueError("Cannot build model with num_numeric_features <= 0")

    sequence_input = Input(shape=(max_sequence_length, num_numeric_features), name='Sequence_Input')
    company_input = Input(shape=(1,), name='Company_Input')

    company_emb = Embedding(input_dim=num_companies, output_dim=embedding_dim,
                            embeddings_initializer='he_normal', name='Company_Embedding')(company_input)
    company_emb_flat = Flatten(name='Flatten_Embedding')(company_emb)

    masked_input = Masking(mask_value=0.0, name='Masking')(sequence_input)
    lstm_layer1 = LSTM(lstm_units_l1, kernel_initializer='he_normal', return_sequences=True, name='LSTM_Layer_1')(masked_input)
    lstm_layer1 = Dropout(dropout_rate, name='Dropout_LSTM1')(lstm_layer1)
    lstm_layer2 = LSTM(lstm_units_l2, kernel_initializer='he_normal', return_sequences=False, name='LSTM_Layer_2')(lstm_layer1)
    lstm_layer2 = Dropout(dropout_rate, name='Dropout_LSTM2')(lstm_layer2)

    merged = Concatenate(name='Concatenate_LSTM_Embedding')([lstm_layer2, company_emb_flat])

    x = Dense(dense_units, activation='relu', kernel_initializer='he_normal', name='Dense_1')(merged)
    x = Dropout(dropout_rate, name='Dropout_Dense1')(x)

    output = Dense(num_targets_out, name='Output_Layer', activation='linear')(x)

    model = Model(inputs=[sequence_input, company_input], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='mse',
                  metrics=[RootMeanSquaredError(name='rmse')])
    return model


# --- Custom Training Loop (Expanding Window) ---
print("\n--- 17. Starting Expanding Window Training ---")
results = {}
all_models = {}
unique_years = sorted(final_df_scaled_targets['Year'].unique())
print(f"Unique years found: {unique_years}")
print(f"Minimum history required: {MIN_HISTORY_YEARS} years.")

first_test_year_index = MIN_HISTORY_YEARS
if first_test_year_index >= len(unique_years):
    print(f"Not enough distinct years ({len(unique_years)}) to perform training with min history {MIN_HISTORY_YEARS}.")
else:
    print(f"Training will start with test year {int(unique_years[first_test_year_index])}")

    # Outer loop: Iterate through years
    for test_year_index in range(first_test_year_index, len(unique_years)):
        test_year = unique_years[test_year_index]
        train_end_year = unique_years[test_year_index - 1]
        print(f"\n{'='*15} Processing Fold: Test Year = {int(test_year)} (Train up to {int(train_end_year)}) {'='*15}")

        train_df = final_df_scaled_targets[final_df_scaled_targets['Year'] <= train_end_year].copy()
        test_potential_df = final_df_scaled_targets[final_df_scaled_targets['Year'] <= test_year].copy()

        print(f"Train df shape: {train_df.shape}, Test potential df shape: {test_potential_df.shape}")

        print("Creating training sequences...")
        X_seq_train, X_cid_train, y_train_scaled_all, max_seq_len_fold, _ = create_sequences(
            train_df, numeric_features_for_lstm, target_cols, 'Company_ID_Encoded', 'Year', min_hist_years=MIN_HISTORY_YEARS)

        if X_seq_train.shape[0] == 0 or max_seq_len_fold == 0:
            print(f"No valid training sequences generated for fold ending {int(train_end_year)}. Skipping fold.")
            continue
        print(f"Generated {X_seq_train.shape[0]} training sequences. Max length: {max_seq_len_fold}. Features: {X_seq_train.shape[-1]}.")

        print("Creating test sequences...")
        test_sequences_hist, test_company_ids, test_actuals_scaled_all = [], [], []
        test_original_indices = []

        for company_id, group in test_potential_df.groupby('Company_ID_Encoded'):
            if test_year in group['Year'].values:
                group_hist = group[group['Year'] <= train_end_year]
                if not group_hist.empty and len(group_hist) >= MIN_HISTORY_YEARS:
                    current_sequence_hist = group_hist[numeric_features_for_lstm].values
                    target_row = group[group['Year'] == test_year]
                    actual_targets_scaled = target_row[target_cols].values.flatten()
                    original_idx = target_row.index.item()

                    if np.isnan(current_sequence_hist).any() or np.isinf(current_sequence_hist).any(): continue
                    if np.isnan(actual_targets_scaled).any() or np.isinf(actual_targets_scaled).any(): continue

                    test_sequences_hist.append(current_sequence_hist)
                    test_company_ids.append(company_id)
                    test_actuals_scaled_all.append(actual_targets_scaled)
                    test_original_indices.append(original_idx)

        if not test_sequences_hist:
            print(f"No valid test sequences generated for year {int(test_year)}. Skipping prediction for this fold.")
            continue

        print(f"Padding {len(test_sequences_hist)} test sequences to max_len={max_seq_len_fold}...")
        X_seq_test = pad_sequences(test_sequences_hist, maxlen=max_seq_len_fold, dtype='float32', padding='pre', truncating='pre', value=0.0)
        print("Padding complete.")
        X_cid_test = np.array(test_company_ids, dtype='int32')
        y_test_actual_scaled_all = np.array(test_actuals_scaled_all, dtype='float32')
        test_original_indices_arr = np.array(test_original_indices)

        print(f"Prepared {X_seq_test.shape[0]} valid test sequences for year {int(test_year)}.")

        # Inner loop: Train separate model for each target
        results[test_year] = {}
        fold_models = {}

        for target_idx, target_name in enumerate(target_cols):
            print(f"\n--- Training for Target: {target_name} (Test Year: {int(test_year)}) ---")

            y_train_target_scaled = y_train_scaled_all[:, target_idx]
            y_test_actual_target_scaled = y_test_actual_scaled_all[:, target_idx]

            if target_name not in target_scalers or target_scalers[target_name] is None:
                print(f"Scaler for {target_name} not found or target was unscalable. Skipping training.")
                results[test_year][target_name] = {'predictions_original': np.full(len(X_cid_test), np.nan),
                                                   'actuals_original': np.full(len(X_cid_test), np.nan),
                                                   'original_indices': test_original_indices_arr}
                continue

            current_n_features = X_seq_train.shape[-1]
            if current_n_features == 0:
                  print("Error: Training data has 0 features. Skipping model.")
                  continue

            model = build_lstm_model(max_seq_len_fold, current_n_features, num_companies, 1, # Output dim = 1
                                     embedding_dim=EMBEDDING_DIM, lstm_units_l1=LSTM_UNITS_L1, lstm_units_l2=LSTM_UNITS_L2,
                                     dense_units=DENSE_UNITS, dropout_rate=DROPOUT_RATE, learning_rate=LEARNING_RATE)

            if test_year_index == first_test_year_index and target_idx == 0:
                 print("\nModel Summary:")
                 model.summary(line_length=100)

            early_stopping = EarlyStopping(monitor='val_rmse', mode='min', patience=EARLY_STOPPING_PATIENCE,
                                           restore_best_weights=True, verbose=0)
            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', mode='min', factor=0.3, patience=REDUCE_LR_PATIENCE,
                                          min_lr=1e-7, verbose=0)

            print(f"Training model for {target_name} with {X_seq_train.shape[0]} sequences...")
            history = model.fit([X_seq_train, X_cid_train.reshape(-1, 1)], y_train_target_scaled,
                                epochs=EPOCHS,
                                batch_size=BATCH_SIZE,
                                validation_split=VALIDATION_SPLIT,
                                callbacks=[early_stopping, reduce_lr],
                                verbose=0)

            epochs_trained = len(history.history['loss'])
            best_val_rmse = min(history.history.get('val_rmse', [np.inf]))
            final_train_rmse = history.history.get('rmse', [np.inf])[-1]
            print(f"Training complete ({epochs_trained} epochs). Final Train RMSE: {final_train_rmse:.4f}, Best Val RMSE (Scaled): {best_val_rmse:.4f}")

            print(f"Predicting for {target_name} on {X_seq_test.shape[0]} test sequences...")
            predictions_scaled = model.predict([X_seq_test, X_cid_test.reshape(-1, 1)], batch_size=BATCH_SIZE*2, verbose=0)

            scaler_t = target_scalers[target_name]
            try:
                 predictions_original = scaler_t.inverse_transform(predictions_scaled).flatten()
                 y_test_actual_original = scaler_t.inverse_transform(y_test_actual_target_scaled.reshape(-1, 1)).flatten()
            except Exception as e:
                 print(f"ERROR during inverse transform for {target_name}: {e}. Storing NaNs.")
                 predictions_original = np.full(predictions_scaled.shape[0], np.nan)
                 try:
                    y_test_actual_original = scaler_t.inverse_transform(y_test_actual_target_scaled.reshape(-1, 1)).flatten()
                 except:
                    y_test_actual_original = np.full(y_test_actual_target_scaled.shape[0], np.nan)

            results[test_year][target_name] = {
                'predictions_original': predictions_original,
                'actuals_original': y_test_actual_original,
                'original_indices': test_original_indices_arr
                }

            valid_preds_mask = ~np.isnan(predictions_original) & ~np.isnan(y_test_actual_original)
            if np.sum(valid_preds_mask) > 0:
                fold_rmse_orig = np.sqrt(mean_squared_error(y_test_actual_original[valid_preds_mask], predictions_original[valid_preds_mask]))
                print(f"  --> Test RMSE for {target_name} (Original Scale, Year {int(test_year)}): {fold_rmse_orig:.4f}")
            else:
                fold_rmse_orig = np.nan
                print(f"  --> Test RMSE for {target_name} (Original Scale, Year {int(test_year)}): NaN (No valid pairs)")

            # fold_models[target_name] = model
            # all_models[(test_year, target_name)] = model

            del model, history
            tf.keras.backend.clear_session()
            gc.collect()

# --- Post-Loop Analysis (Overall Performance per Target) ---
print(f"\n{'='*20} Training Loop Finished {'='*20}")

all_predictions_df_list = [] # To store results in a structured way

if results:
    print("\n--- Calculating Overall Performance Per Target (Original Scale) ---")
    overall_rmse_results = {} # Store final RMSE per target

    for target_idx, target_name in enumerate(target_cols):
        print(f"\n--- Overall Performance for Target: {target_name} ---")
        all_actuals_orig_list = []
        all_predictions_orig_list = []
        fold_details = []
        nan_folds_report = []

        for year in sorted(results.keys()):
            if year in results and target_name in results[year]:
                data = results[year][target_name]
                pred_orig = data['predictions_original']
                act_orig = data['actuals_original']
                orig_indices = data['original_indices']

                is_pred_invalid = np.all(np.isnan(pred_orig)) or np.all(np.isinf(pred_orig))
                is_act_invalid = np.all(np.isnan(act_orig)) or np.all(np.isinf(act_orig))

                if is_pred_invalid or is_act_invalid:
                    nan_folds_report.append(int(year))
                    print(f"    Skipping year {int(year)} for overall {target_name} calculation due to invalid predictions/actuals.")
                else:
                    valid_mask = ~np.isnan(pred_orig) & ~np.isnan(act_orig) & ~np.isinf(pred_orig) & ~np.isinf(act_orig)
                    if np.sum(valid_mask) > 0:
                         all_actuals_orig_list.append(act_orig[valid_mask])
                         all_predictions_orig_list.append(pred_orig[valid_mask])

                         fold_df = pd.DataFrame({
                             'Original_Index': orig_indices[valid_mask],
                             'Test_Year': int(year),
                             'Target': target_name,
                             f'{target_name}_Actual': act_orig[valid_mask],
                             f'{target_name}_Predicted': pred_orig[valid_mask]
                         })
                         all_predictions_df_list.append(fold_df)
                    else:
                         nan_folds_report.append(int(year))
                         print(f"    Skipping year {int(year)} for overall {target_name} calculation as no valid prediction pairs found.")

        # Calculate overall metrics
        final_rmse = np.nan # Default to NaN
        final_mae = np.nan
        valid_point_count = 0
        if all_actuals_orig_list:
            all_actuals_np = np.concatenate(all_actuals_orig_list)
            all_predictions_np = np.concatenate(all_predictions_orig_list)

            final_valid_mask = ~np.isnan(all_actuals_np) & ~np.isnan(all_predictions_np) & ~np.isinf(all_actuals_np) & ~np.isinf(all_predictions_np)
            if np.sum(final_valid_mask) > 0:
                final_actuals = all_actuals_np[final_valid_mask]
                final_predictions = all_predictions_np[final_valid_mask]
                valid_point_count = len(final_actuals)

                final_mse = mean_squared_error(final_actuals, final_predictions)
                final_rmse = np.sqrt(final_mse)
                final_mae = np.mean(np.abs(final_predictions - final_actuals))

                print(f"  Final Overall RMSE (Original Scale): {final_rmse:.4f}")
                print(f"  Final Overall MAE (Original Scale): {final_mae:.4f}")
                print(f"  (Based on {valid_point_count} valid prediction points across all folds)")
            else:
                print("  No valid prediction points found across all folds after filtering.")

            if nan_folds_report:
                print(f"  Issues (NaNs/No valid pairs) occurred for this target in test years: {sorted(list(set(nan_folds_report)))}")
        else:
            print(f"  No valid predictions generated across any fold for {target_name}.")

        # Store the final RMSE for summary display
        overall_rmse_results[target_name] = final_rmse

else:
    print("No results generated during the training loop.")
    overall_rmse_results = {t: np.nan for t in target_cols}


# --- Combine all predictions into a single DataFrame (Optional Display) ---
if all_predictions_df_list:
    print("\n--- Creating Final Predictions DataFrame ---")
    # Set index directly during concat/merge if possible, otherwise afterwards
    predictions_results_df_list_indexed = [df.set_index('Original_Index') for df in all_predictions_df_list if 'Original_Index' in df.columns]

    if predictions_results_df_list_indexed:
        # Concatenate DataFrames that were successfully indexed
        predictions_results_df = pd.concat(predictions_results_df_list_indexed)

        # Merge with original data to get Company/Year context
        original_context_df = final_df_scaled_targets[['Company', 'Year']].reset_index().rename(columns={'index': 'Original_Index'})
        original_context_df = original_context_df.set_index('Original_Index') # Set index for merge

        # Perform the merge - use left join to keep all predictions
        # Ensure indices are compatible (both named 'Original_Index')
        predictions_results_df = predictions_results_df.merge(original_context_df, on='Original_Index', how='left')


        print("Sample of combined prediction results:")
        print(predictions_results_df.head())
        print(f"Shape of combined results: {predictions_results_df.shape}")
    else:
        print("Could not create combined DataFrame (issue with indices).")
        predictions_results_df = pd.DataFrame() # Assign empty df
else:
    print("\nNo prediction results to combine into a DataFrame.")
    predictions_results_df = pd.DataFrame() # Assign empty df


# --- FINAL OVERALL RMSE SUMMARY ---
print(f"\n{'='*20} FINAL OVERALL RMSE SUMMARY {'='*20}")
print("Average RMSE across all test years (original scale):")
for target_name, rmse_value in overall_rmse_results.items():
    if pd.isna(rmse_value):
        print(f"  - {target_name}: NaN (No valid predictions)")
    else:
        print(f"  - {target_name}: {rmse_value:.4f}")
print(f"{'='*58}")

# --- Feature Importance / Attribution (Placeholder) ---
# (Keep placeholder as is)
print("\n--- Feature Importance Analysis (Conceptual) ---")
# ... (rest of the placeholder info) ...
if 'pca_model' in locals() and pca_model:
    print("\nPCA Explained Variance per Component:")
    print(pca_model.explained_variance_ratio_)
    print(f"Total variance explained by {n_components} components: {pca_model.explained_variance_ratio_.sum():.4f}")

# --- Visualizations (Placeholder & Example) ---
# (Keep placeholder and example plot as is)
print("\n--- Visualization Ideas (Conceptual) ---")
# ... (rest of the placeholder info) ...
if 'predictions_results_df' in locals() and not predictions_results_df.empty:
     first_target = target_cols[0]
     actual_col = f'{first_target}_Actual'
     predicted_col = f'{first_target}_Predicted'

     # Check if required columns exist after merge/pivot
     if actual_col in predictions_results_df.columns and predicted_col in predictions_results_df.columns:
         # Drop NaNs just for plotting this specific target pair
         plot_df = predictions_results_df[[actual_col, predicted_col]].dropna()
         if not plot_df.empty:
             print(f"\n--- Generating Actual vs Predicted Plot for {first_target} ---")
             plt.figure(figsize=(8, 8))
             sns.scatterplot(data=plot_df, x=actual_col, y=predicted_col, alpha=0.6)
             # Add identity line (y=x)
             min_val = min(plot_df[actual_col].min(), plot_df[predicted_col].min())
             max_val = max(plot_df[actual_col].max(), plot_df[predicted_col].max())
             plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
             plt.title(f'Actual vs. Predicted (Original Scale) - {first_target}')
             plt.xlabel('Actual Values')
             plt.ylabel('Predicted Values')
             plt.grid(True)
             plt.show()
         else:
              print(f"No valid (non-NaN) data points to plot for {first_target}.")
     else:
          print(f"Could not find columns '{actual_col}' or '{predicted_col}' in results DataFrame for plotting.")


print("\n--- Script Complete ---")









