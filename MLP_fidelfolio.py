# -*- coding: utf-8 -*-
"""MLP_FIDELFOLIO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHsdWnyvm87h6Ul6A6jrwQbTC7yXEs7Q
"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

df = pd.read_csv('/content/FidelFolio_Dataset.csv')



df.shape

df.info()

df.isnull().sum()

df.columns

df

df.describe()

df.head()

df['Company'].value_counts()

num_companies



pip install tensorflow

"""USING MLP"""

# --- [IMPORTS, CONFIGURATION as before]
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA # Keep PCA
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding, # Keep Embedding, Dense, Input
                                     Concatenate, Masking, Flatten, Dropout) # Keep Flatten, Concatenate, Dropout

from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
import gc

# --- Configuration ---
# Preprocessing
N_NEIGHBORS_IMPUTE = 5
Z_SCORE_CAP = 3
PCA_VARIANCE_RATIO = 0.95
# Model Hyperparameters
EMBEDDING_DIM = 16
# MLP Specific - define layer sizes
MLP_L1_UNITS = 128        # Units in first MLP hidden layer
MLP_L2_UNITS = 64         # Units in second MLP hidden layer
DENSE_UNITS = MLP_L2_UNITS # Use L2 size for consistency if needed later, otherwise remove
DROPOUT_RATE = 0.3       # Dropout rate might need adjustment for MLP
LEARNING_RATE = 1e-4
# Training Parameters
EPOCHS = 75
BATCH_SIZE = 32
VALIDATION_SPLIT = 0.15
EARLY_STOPPING_PATIENCE = 12
REDUCE_LR_PATIENCE = 6
MIN_HISTORY_YEARS = 2

warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
tf.get_logger().setLevel('WARN')
tf.autograph.set_verbosity(0)

# --- Load Data ---

# df = pd.read_csv('your_data.csv')
print("--- Initial Data Sample (from df) ---")
print(df.head())
print(f"\nInitial DataFrame shape: {df.shape}")

# --- Preprocessing ---


# 1. Clean column names
print("\n--- 1. Cleaning Column Names ---")
df.columns = df.columns.str.strip()

# 2. Convert object columns to numeric
print("\n--- 2. Converting Columns to Numeric ---")

potential_numeric_cols = df.select_dtypes(include='object').columns
if 'Company' in potential_numeric_cols: potential_numeric_cols = potential_numeric_cols.drop('Company')
converted_count = 0
for col in potential_numeric_cols:
    if col not in df.columns: continue
    try:
        original_type = df[col].dtype
        converted_col = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce')
        if pd.api.types.is_numeric_dtype(converted_col) and converted_col.notnull().sum() > 0:
             df[col] = converted_col
             if df[col].dtype != original_type: converted_count += 1
    except Exception: pass
print(f"Attempted numeric conversion. {converted_count} columns changed type.")

# ---  Step 3: Feature Engineering (Generic Differences) ---
print("\n--- 3. Feature Engineering (Generic Differences) ---")
target_cols = ['Target 1', 'Target 2', 'Target 3']
engineered_features = []

# Identify original numeric features BEFORE creating differences
original_numeric_cols = df.select_dtypes(include=np.number).columns.drop(
    target_cols + ['Year', 'Company_ID'], errors='ignore' # Exclude targets, Year, and maybe old Company_ID
).tolist()

print(f"Calculating YoY differences for {len(original_numeric_cols)} original numeric features...")
df = df.sort_values(by=['Company', 'Year']) # CRITICAL: Sort before using shift/diff

for col in original_numeric_cols:
    # Ensure the original column is numeric first
    df[col] = pd.to_numeric(df[col], errors='coerce')
    # Calculate absolute difference
    diff_col_name = f'FE_{col}_Diff'
    df[diff_col_name] = df.groupby('Company')[col].diff()
    # Convert to float32 and add to list for processing
    df[diff_col_name] = df[diff_col_name].astype(np.float32)
    engineered_features.append(diff_col_name)

print(f" - Created {len(engineered_features)} absolute difference (YoY) features.")

# --- [MODIFIED] Step 4: Separate Columns & Identify ALL Features for Processing ---
print("\n--- 4. Separating Columns & Identifying Processing Features ---")
company_col_preserved = df['Company'].copy()
year_col_preserved = df['Year'].copy()
target_df_preserved = df[target_cols].copy() # Preserve original targets

# Start with the original numeric cols + newly engineered diff cols
numeric_processing_cols = original_numeric_cols + engineered_features

# Remove any duplicates just in case, and ensure they exist
numeric_processing_cols = sorted(list(set(
    col for col in numeric_processing_cols if col in df.columns
)))

# --- Final Sanity Checks ---
if 'Year' in numeric_processing_cols: raise ValueError("FATAL: 'Year' included for processing!")
if 'Company' in numeric_processing_cols: raise ValueError("FATAL: 'Company' included for processing!")
if any(tc in numeric_processing_cols for tc in target_cols): raise ValueError("FATAL: Targets included for processing!")
# --- End Sanity Checks ---

numeric_df_to_process = df[numeric_processing_cols].copy()
print(f"{len(numeric_processing_cols)} numeric columns identified for processing (Original + Engineered Diffs).")

# 5. KNN Imputation (Features + Engineered Features)
print("\n--- 5. Performing KNN Imputation ---")
# (Impute the combined set of original features and difference features)
if not numeric_df_to_process.empty and numeric_df_to_process.isnull().any().any():
    null_counts_before = numeric_df_to_process.isnull().sum()
    print(f"Nulls before imputation in {null_counts_before[null_counts_before > 0].count()} columns (Total: {null_counts_before.sum()})")
    imputer = KNNImputer(n_neighbors=N_NEIGHBORS_IMPUTE)
    numeric_df_to_process = numeric_df_to_process.astype(np.float32)
    imputed_array = imputer.fit_transform(numeric_df_to_process)
    numeric_df_processed = pd.DataFrame(imputed_array, columns=numeric_processing_cols, index=numeric_df_to_process.index)
    if numeric_df_processed.isnull().sum().sum() > 0: print("WARNING: Nulls remain after KNN imputation!")
    else: print("Imputation complete.")
else:
    print("No missing values found in processing columns or no columns to process.")
    numeric_df_processed = numeric_df_to_process

# 6. Robust Scaling (Features + Engineered Features)
print("\n--- 6. Applying Robust Scaler ---")
cols_to_scale = numeric_processing_cols
if cols_to_scale and not numeric_df_processed.empty:
    feature_scaler = RobustScaler()
    scaled_data = feature_scaler.fit_transform(numeric_df_processed[cols_to_scale])
    numeric_df_scaled = pd.DataFrame(scaled_data, columns=cols_to_scale, index=numeric_df_processed.index)
    print(f"Scaled {len(cols_to_scale)} columns (Original + Diffs) using RobustScaler.")
else:
    print("No columns identified or available for scaling.")
    numeric_df_scaled = numeric_df_processed

# 7. Capping Outliers (on Scaled Features + Engineered Features)
print("\n--- 7. Capping Outliers ---")
# (Cap the combined set)
if cols_to_scale and not numeric_df_scaled.empty:
    numeric_df_capped = numeric_df_scaled.copy()
    capped_cols_count = 0
    for col in cols_to_scale:
        col_data = numeric_df_capped[col].dropna();
        if not col_data.empty:
            col_mean=col_data.mean(); col_std=col_data.std()
            if pd.notna(col_std) and col_std > 1e-6:
                lower=col_mean-Z_SCORE_CAP*col_std; upper=col_mean+Z_SCORE_CAP*col_std
                min_orig=numeric_df_capped[col].min(); max_orig=numeric_df_capped[col].max()
                numeric_df_capped[col]=numeric_df_capped[col].clip(lower, upper)
                if numeric_df_capped[col].min()>min_orig or numeric_df_capped[col].max()<max_orig: capped_cols_count+=1
    if capped_cols_count > 0: print(f"Capped outliers in {capped_cols_count} columns.")
    else: print("No outliers needed capping.")
    numeric_df_final_processed = numeric_df_capped
else:
    print("No columns available for capping.")
    numeric_df_final_processed = numeric_df_scaled

# 8. Apply PCA (on combined imputed, scaled, capped features)
print(f"\n--- 8. Applying PCA (Retaining {PCA_VARIANCE_RATIO*100:.0f}% Variance) ---")
n_original_features = numeric_df_final_processed.shape[1]
if n_original_features > 0:
    pca = PCA(n_components=PCA_VARIANCE_RATIO)
    pca_components = pca.fit_transform(numeric_df_final_processed)
    n_components = pca.n_components_
    print(f"PCA applied. Reduced features from {n_original_features} to {n_components}.")
    pc_cols = [f'PC_{i+1}' for i in range(n_components)]
    pca_df = pd.DataFrame(pca_components, columns=pc_cols, index=numeric_df_final_processed.index)
    numeric_features_for_lstm = pc_cols 
else:
    print("Skipping PCA: No numeric features to process.")
    pca_df = pd.DataFrame(index=df.index)
    numeric_features_for_lstm = []
    n_components = 0

# 9. Reconstruct DataFrame (Using PCA Components)
print("\n--- 9. Reconstructing DataFrame with PCA Components ---")
final_df = pd.concat([
    company_col_preserved.reset_index(drop=True),
    year_col_preserved.reset_index(drop=True),
    pca_df.reset_index(drop=True),             # PCA components
    target_df_preserved.reset_index(drop=True) # Original Targets
], axis=1)

# 10. Scale Target Variables
print("\n--- 10. Scaling Target Variables (Separately) ---")
# (Same code as before to scale targets and store scalers)
target_scalers = {}
final_df_scaled_targets = final_df.copy()
for target_col in target_cols:
    if target_col not in final_df_scaled_targets.columns: continue
    scaler = StandardScaler()
    target_data = final_df_scaled_targets[[target_col]].astype(np.float32)
    valid_target_data = target_data.dropna()
    if not valid_target_data.empty:
        scaler.fit(valid_target_data)
        final_df_scaled_targets[target_col] = scaler.transform(target_data)
        target_scalers[target_col] = scaler
        print(f"Applied StandardScaler to target column: '{target_col}'")
    else: target_scalers[target_col] = None

# 11. Label Encode Company
print("\n--- 11. Encoding Company Names ---")
le = LabelEncoder()
final_df_scaled_targets['Company_ID_Encoded'] = le.fit_transform(final_df_scaled_targets['Company'])
num_companies = final_df_scaled_targets['Company_ID_Encoded'].nunique()
print(f"Encoded 'Company' into 'Company_ID_Encoded'. Found {num_companies} unique companies.")

print("\n--- Final Preprocessed Data Sample (PCA Features, Scaled Targets) ---")
print(final_df_scaled_targets.head())
print(f"\nFinal DataFrame shape: {final_df_scaled_targets.shape}")
# Final check for NaNs in PCA components
if not numeric_features_for_lstm: print("No PCA features to check.")
elif final_df_scaled_targets[numeric_features_for_lstm].isnull().any().any():
      print(f"\nWARNING: NaNs detected in final PCA columns:\n{final_df_scaled_targets[numeric_features_for_lstm].isnull().sum().sort_values(ascending=False).head()}")



# 12. Sort Data
print("\n--- 12. Sorting Data ---")
final_df_scaled_targets = final_df_scaled_targets.sort_values(by=['Company_ID_Encoded', 'Year']).reset_index(drop=True)

# 13. Define LSTM Features (PCA Components used by sequence generator)
print("\n--- 13. Defining Features for Sequence Input (Using PCA Components) ---")
print(f"Features for sequence input ({len(numeric_features_for_lstm)}): {numeric_features_for_lstm[:5]}...")

# 14. Sequence Creation Function (Keep as is)
print("\n--- 14. Defining Sequence Creation Function ---")
# (Function definition remains the same - it creates 3D sequence arrays)
def create_sequences(df, numeric_cols, target_cols, company_col_id_encoded, year_col, min_hist_years=1):
    # ... (exact same function implementation as the previous corrected answer) ...
    all_sequences, all_company_ids, all_targets = [], [], []
    max_len = 0; skipped_nan_sequences, skipped_nan_targets = 0, 0
    company_groups = df.groupby(company_col_id_encoded)
    for company_id, group in company_groups:
        group = group.sort_values(year_col); features = group[numeric_cols].values; targets = group[target_cols].values
        for i in range(min_hist_years, len(group)):
            current_sequence = features[:i, :]; current_target = targets[i, :] # Get all 3 scaled targets
            if np.isnan(current_sequence).any() or np.isinf(current_sequence).any(): skipped_nan_sequences += 1; continue
            if np.isnan(current_target).any() or np.isinf(current_target).any(): skipped_nan_targets += 1; continue
            all_sequences.append(current_sequence); all_company_ids.append(company_id); all_targets.append(current_target)
            if current_sequence.shape[0] > max_len: max_len = current_sequence.shape[0]
    if skipped_nan_sequences > 0: print(f"Warning: Skipped {skipped_nan_sequences} sequences (NaN features).")
    if skipped_nan_targets > 0: print(f"Warning: Skipped {skipped_nan_targets} sequences (NaN targets).")
    if not all_sequences: return np.array([]), np.array([]), np.array([]), 0
    X_padded = pad_sequences(all_sequences, maxlen=max_len, dtype='float32', padding='pre', truncating='pre')
    y_array = np.array(all_targets, dtype='float32'); X_comp_array = np.array(all_company_ids, dtype='int32')
    if np.isnan(X_padded).any(): print("FATAL WARNING: NaNs in X_padded!")
    if np.isnan(y_array).any(): print("FATAL WARNING: NaNs in y_array!")
    return X_padded, X_comp_array, y_array, max_len


# --- [MODIFIED] Model Definition (MLP with 2 Hidden Layers) ---
print("\n--- 15. Defining MLP Model Architecture Function ---")
def build_mlp_model(max_sequence_length, num_numeric_features, num_companies, num_targets_out, # num_targets_out will be 1
                    embedding_dim=EMBEDDING_DIM, mlp_l1_units=MLP_L1_UNITS, mlp_l2_units=MLP_L2_UNITS,
                    dropout_rate=DROPOUT_RATE, learning_rate=LEARNING_RATE):
    """Builds a 2-hidden-layer MLP model that accepts sequence input (by flattening)."""

    # Input layer for sequences (will be flattened)
    sequence_input = Input(shape=(max_sequence_length, num_numeric_features), name='Sequence_Input')
    # Input layer for company ID
    company_input = Input(shape=(1,), name='Company_Input')

    # --- Flatten the sequence input ---
    # This converts (batch, timesteps, features) to (batch, timesteps * features)
    # Note: This treats padded steps the same as real steps after flattening.
    flattened_sequence = Flatten(name='Flatten_Sequence')(sequence_input)

    # --- Company Embedding Branch ---
    company_emb = Embedding(input_dim=num_companies, output_dim=embedding_dim,
                            embeddings_initializer='he_normal', name='Company_Embedding')(company_input)
    company_emb_flat = Flatten(name='Flatten_Embedding')(company_emb)

    # --- Combine Flattened Sequence and Company Embedding ---
    merged = Concatenate(name='Concatenate_FlatSeq_Embedding')([flattened_sequence, company_emb_flat])

    # --- MLP Layers ---
    # Hidden Layer 1
    x = Dense(mlp_l1_units, activation='relu', kernel_initializer='he_normal', name='MLP_Hidden_1')(merged)
    x = Dropout(dropout_rate)(x)
    # Hidden Layer 2
    x = Dense(mlp_l2_units, activation='relu', kernel_initializer='he_normal', name='MLP_Hidden_2')(x)
    x = Dropout(dropout_rate)(x)

    # Output Layer (Single neuron for regression per target)
    output = Dense(num_targets_out, name='Output_Layer')(x)

    # --- Build and Compile Model ---
    model = Model(inputs=[sequence_input, company_input], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='mse',
                  metrics=[RootMeanSquaredError(name='rmse')])
    return model


# --- Custom Training Loop (Using MLP Model) ---
print("\n--- 16. Starting Expanding Window Training (MLP Model) ---")
results = {}
unique_years = sorted(final_df_scaled_targets['Year'].unique())
print(f"Unique years found: {unique_years}")

if len(unique_years) <= MIN_HISTORY_YEARS:
    print(f"Not enough distinct years ({len(unique_years)}) for training.")
else:
    # Outer loop: Iterate through years
    for test_year_index in range(MIN_HISTORY_YEARS, len(unique_years)):
        test_year = unique_years[test_year_index]; train_end_year = unique_years[test_year_index - 1]
        print(f"\n{'='*10} Processing Fold for Test Year: {int(test_year)} (Train up to {int(train_end_year)}) {'='*10}")

        # 1. Filter & Create base sequences (using PCA columns)
        train_df = final_df_scaled_targets[final_df_scaled_targets['Year'] <= train_end_year].copy()
        X_seq_train, X_cid_train, y_train_scaled_all, max_seq_len_train = create_sequences(
            train_df, numeric_features_for_lstm, target_cols, 'Company_ID_Encoded', 'Year', min_hist_years=MIN_HISTORY_YEARS)
        if X_seq_train.shape[0] == 0 or max_seq_len_train == 0: print(f"No valid training sequences generated. Skipping fold."); continue
        print(f"Generated {X_seq_train.shape[0]} base training sequences (Input: {X_seq_train.shape[-1]} PCs, Max Len: {max_seq_len_train}).")

        # 2. Prepare base test sequences
        test_sequences, test_company_ids, test_actuals_scaled_all = [], [], []
        # ... (same NaN-checking loop as before) ...
        test_candidate_df=final_df_scaled_targets[final_df_scaled_targets['Year']<=test_year].copy()
        for company_id, group in test_candidate_df.groupby('Company_ID_Encoded'):
            if test_year in group['Year'].values:
                 group_hist=group[group['Year']<=train_end_year]
                 if not group_hist.empty and len(group_hist)>=MIN_HISTORY_YEARS:
                     current_sequence=group_hist[numeric_features_for_lstm].values
                     actual_targets_scaled=group[group['Year']==test_year][target_cols].values.flatten()
                     if np.isnan(current_sequence).any() or np.isinf(current_sequence).any(): continue
                     if np.isnan(actual_targets_scaled).any() or np.isinf(actual_targets_scaled).any(): continue
                     test_sequences.append(current_sequence); test_company_ids.append(company_id); test_actuals_scaled_all.append(actual_targets_scaled)
        if not test_sequences: print(f"No valid test sequences generated. Skipping prediction."); continue
        X_seq_test = pad_sequences(test_sequences, maxlen=max_seq_len_train, dtype='float32', padding='pre', truncating='pre')
        X_cid_test = np.array(test_company_ids, dtype='int32')
        y_test_actual_scaled_all = np.array(test_actuals_scaled_all, dtype='float32')
        print(f"Prepared {X_seq_test.shape[0]} base valid test sequences (Input: {X_seq_test.shape[-1]} PCs).")

        # --- Inner loop: Train separate MLP model for each target ---
        results[test_year] = {}
        for target_idx, target_name in enumerate(target_cols):
            print(f"\n--- Training MLP for Target: {target_name} (Year: {int(test_year)}) ---")

            y_train_target_scaled = y_train_scaled_all[:, target_idx]
            y_test_actual_target_scaled = y_test_actual_scaled_all[:, target_idx]
            if target_name not in target_scalers or target_scalers[target_name] is None: print(f"Scaler for {target_name} not found. Skipping."); continue

            # Build MLP model (passing n_components)
            current_n_features = n_components if numeric_features_for_lstm else 0
            if current_n_features == 0: print("No PCA features to train on. Skipping model."); continue

            # Call the new MLP builder function
            model = build_mlp_model(
                max_sequence_length=max_seq_len_train,
                num_numeric_features=current_n_features, # Features per time step (used by Flatten)
                num_companies=num_companies,
                num_targets_out=1 # Single output for this target
            )
            # Optional: Print summary only once
            # if test_year_index == MIN_HISTORY_YEARS and target_idx == 0: model.summary()


            # Define Callbacks
            early_stopping = EarlyStopping(monitor='val_rmse', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=0)
            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.2, patience=REDUCE_LR_PATIENCE, min_lr=1e-6, verbose=0)

            # Train model
            print(f"Training MLP for {target_name}...")
            history = model.fit([X_seq_train, X_cid_train.reshape(-1, 1)], y_train_target_scaled, # Still provide sequence input
                                epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,
                                callbacks=[early_stopping, reduce_lr], verbose=0)
            epochs_trained = len(history.history['loss']); best_val_rmse = min(history.history.get('val_rmse', [np.inf]))
            print(f"Training complete ({epochs_trained} epochs). Best Val RMSE (Scaled): {best_val_rmse:.4f}")

            # Predict (scaled)
            predictions_scaled = model.predict([X_seq_test, X_cid_test.reshape(-1, 1)], verbose=0) # Still provide sequence input

            # Inverse Transform
            scaler_t = target_scalers[target_name]
            try:
                 predictions_original = scaler_t.inverse_transform(predictions_scaled).flatten()
                 y_test_actual_original = scaler_t.inverse_transform(y_test_actual_target_scaled.reshape(-1, 1)).flatten()
            except Exception as e:
                 print(f"ERROR during inverse transform for {target_name}: {e}. Storing NaNs.")
                 predictions_original = np.full(predictions_scaled.shape[0], np.nan)
                 y_test_actual_original = np.full(y_test_actual_target_scaled.shape[0], np.nan)

            # Store results
            results[test_year][target_name] = {'predictions_original': predictions_original, 'actuals_original': y_test_actual_original}

            # Evaluate fold performance (Original Scale)
            if np.isnan(predictions_original).any() or np.isnan(y_test_actual_original).any(): fold_rmse_orig = np.nan
            else: fold_rmse_orig = np.sqrt(np.mean((predictions_original - y_test_actual_original)**2))
            print(f"  --> Test RMSE for {target_name} (Original Scale): {fold_rmse_orig:.4f}")

            # Clean up memory
            del model; tf.keras.backend.clear_session(); gc.collect()

# --- Post-Loop Analysis (Separated per target) ---
print(f"\n{'='*20} Training Loop Finished {'='*20}")
# (Exact same post-loop analysis code as previous answer to print RMSE/MAE per target)
if results:
    print("\nCalculating Overall Performance Per Target (Original Scale) - MLP Model:")
    for target_idx, target_name in enumerate(target_cols):
        print(f"\n--- Overall Performance for Target: {target_name} ---")
        all_actuals_orig_list = []; all_predictions_orig_list = []; nan_folds_report = []
        for year in sorted(results.keys()):
            if year in results and target_name in results[year]:
                data = results[year][target_name]; pred_orig = data['predictions_original']; act_orig = data['actuals_original']
                if np.isnan(pred_orig).any() or np.isinf(pred_orig).any() or np.isnan(act_orig).any() or np.isinf(act_orig).any():
                    nan_folds_report.append(int(year))
                else: all_actuals_orig_list.append(act_orig); all_predictions_orig_list.append(pred_orig)
        if all_actuals_orig_list:
            all_actuals_np = np.concatenate(all_actuals_orig_list, axis=0); all_predictions_np = np.concatenate(all_predictions_orig_list, axis=0)
            final_mse = np.mean((all_predictions_np - all_actuals_np)**2); final_rmse = np.sqrt(final_mse)
            final_mae = np.mean(np.abs(all_predictions_np - all_actuals_np))
            print(f"  Final Overall RMSE (Original Scale, excluding {len(nan_folds_report)} NaN folds): {final_rmse:.4f}")
            print(f"  Final Overall MAE (Original Scale, excluding {len(nan_folds_report)} NaN folds): {final_mae:.4f}")
            if nan_folds_report: print(f"  NaN results occurred for this target in test years: {nan_folds_report}")
        else: print(f"  No valid (non-NaN) predictions in original scale generated for {target_name}.")
else: print("No results generated.")

print("\n--- Script Complete ---")





